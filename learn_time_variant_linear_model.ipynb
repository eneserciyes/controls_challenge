{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ACC_G = 9.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(data_path)\n",
    "    processed_df = pd.DataFrame(\n",
    "        {\n",
    "            \"roll_lataccel\": np.sin(df[\"roll\"].values) * ACC_G,\n",
    "            \"v_ego\": df[\"vEgo\"].values,\n",
    "            \"a_ego\": df[\"aEgo\"].values,\n",
    "            \"target_lataccel\": df[\"targetLateralAcceleration\"].values,\n",
    "            \"steer_command\": -df[\n",
    "                \"steerCommand\"\n",
    "            ].values,  # steer commands are logged with left-positive convention but this simulator uses right-positive\n",
    "        }\n",
    "    )\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in range(19000):\n",
    "  pd_frame = get_data(f'data/{i:05d}.csv')\n",
    "  td = pd_frame[:100].values\n",
    "  train_data.append(td)\n",
    "\n",
    "train_data = np.array(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = []\n",
    "for i in range(19000, 20000):\n",
    "  pd_frame = get_data(f'data/{i:05d}.csv')\n",
    "  td = pd_frame[:100].values\n",
    "  val_data.append(td)\n",
    "\n",
    "val_data = np.array(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(train_data) # train_data.shape[0]\n",
    "\n",
    "Xtr = torch.tensor(train_data[:data_size], dtype=torch.float32)# .reshape(-1, 5)\n",
    "Xval = torch.tensor(val_data, dtype=torch.float32)# .reshape(-1, 5)\n",
    "\n",
    "mean, std = Xtr.mean(dim=(0,1)), Xtr.std(dim=(0,1))\n",
    "\n",
    "Xtr = (Xtr - mean) / std\n",
    "Xval = (Xval - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytr = (Xtr[:, 1:, 3:4] - Xtr[:, :-1, 3:4]) # delta lateral acceleration\n",
    "Yval = (Xval[:, 1:, 3:4] - Xval[:, :-1, 3:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = Xtr[:, :-1].reshape(-1, 5)\n",
    "Xval = Xval[:, :-1].reshape(-1, 5)\n",
    "\n",
    "Ytr = Ytr.reshape(-1)\n",
    "Yval = Yval.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.9169,  1.1152, -0.1009,  1.7394,  1.5623]), tensor(0.0804))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr[0], Ytr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8997,  1.1145, -0.1498,  1.8198,  1.5829])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(Xtr, Ytr, batch_size: int):\n",
    "    idx = np.random.choice(Xtr.shape[0], batch_size)\n",
    "    return Xtr[idx], Ytr[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LTVDynamicsModel(nn.Module):  # Linear Time-Varying Dynamics Model\n",
    "    def __init__(self, layers=[64, 64]):\n",
    "        super(LTVDynamicsModel, self).__init__()\n",
    "        layers = [nn.Linear(3, layers[0]), nn.ReLU()] + [\n",
    "            nn.Linear(layers[i], layers[i + 1]) for i in range(len(layers) - 1)\n",
    "        ] + [nn.Linear(layers[-1], 3)]\n",
    "        self.time_varying_F = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, xu):\n",
    "        \"\"\"\n",
    "        xu: [roll_lataccel, v_ego, a_ego, lataccel, steer_command]\n",
    "        \"\"\"\n",
    "        x = self.time_varying_F(xu[:, :3]) # Ft(roll_lataccel, v_ego, a_ego)\n",
    "        Ft = x[:, 0:2].reshape(-1, 1, 2)\n",
    "        ft = x[:, 2:].reshape(-1, 1, 1)\n",
    "\n",
    "        xt = xu[:, 3:].reshape(-1, 2, 1) # [x, u]\n",
    "        xt1 = (torch.bmm(Ft, xt) + ft).reshape(-1) # [x_t+1]\n",
    "        return xt1\n",
    "\n",
    "model = LTVDynamicsModel([128, 128, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.cuda()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_train_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.lr = 1e-5\n",
    "num_steps = 100000\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Validation Loss 0.04104582592844963\n",
      "Step 0: Train Loss 0.017528023570775986 | Best Val Loss 0.04104582592844963\n",
      "Step 100: Train Loss 0.0025981850922107697 | Best Val Loss 0.04104582592844963\n",
      "Step 200: Train Loss 0.008357249200344086 | Best Val Loss 0.04104582592844963\n",
      "Step 300: Train Loss 0.01295576710253954 | Best Val Loss 0.04104582592844963\n",
      "Step 400: Train Loss 0.005972393322736025 | Best Val Loss 0.04104582592844963\n",
      "Step 500: Train Loss 0.004852834157645702 | Best Val Loss 0.04104582592844963\n",
      "Step 600: Train Loss 0.004040725063532591 | Best Val Loss 0.04104582592844963\n",
      "Step 700: Train Loss 0.004120287485420704 | Best Val Loss 0.04104582592844963\n",
      "Step 800: Train Loss 0.01531993504613638 | Best Val Loss 0.04104582592844963\n",
      "Step 900: Train Loss 0.1145501434803009 | Best Val Loss 0.04104582592844963\n",
      "Step 1000: Validation Loss 0.38088127970695496\n",
      "Step 1000: Train Loss 0.5004541873931885 | Best Val Loss 0.04104582592844963\n",
      "Step 1100: Train Loss 0.03650283068418503 | Best Val Loss 0.04104582592844963\n",
      "Step 1200: Train Loss 0.013758853077888489 | Best Val Loss 0.04104582592844963\n",
      "Step 1300: Train Loss 0.082094706594944 | Best Val Loss 0.04104582592844963\n",
      "Step 1400: Train Loss 0.14244362711906433 | Best Val Loss 0.04104582592844963\n",
      "Step 1500: Train Loss 0.011007556691765785 | Best Val Loss 0.04104582592844963\n",
      "Step 1600: Train Loss 0.06322509795427322 | Best Val Loss 0.04104582592844963\n",
      "Step 1700: Train Loss 0.01162634789943695 | Best Val Loss 0.04104582592844963\n",
      "Step 1800: Train Loss 0.010205674916505814 | Best Val Loss 0.04104582592844963\n",
      "Step 1900: Train Loss 4.106654644012451 | Best Val Loss 0.04104582592844963\n",
      "Step 2000: Validation Loss 0.025055356323719025\n",
      "Step 2000: Train Loss 0.011478414759039879 | Best Val Loss 0.025055356323719025\n",
      "Step 2100: Train Loss 0.010668089613318443 | Best Val Loss 0.025055356323719025\n",
      "Step 2200: Train Loss 0.0063537778332829475 | Best Val Loss 0.025055356323719025\n",
      "Step 2300: Train Loss 0.010738112032413483 | Best Val Loss 0.025055356323719025\n",
      "Step 2400: Train Loss 0.006941949017345905 | Best Val Loss 0.025055356323719025\n",
      "Step 2500: Train Loss 0.03207928687334061 | Best Val Loss 0.025055356323719025\n",
      "Step 2600: Train Loss 0.14254790544509888 | Best Val Loss 0.025055356323719025\n",
      "Step 2700: Train Loss 0.19567815959453583 | Best Val Loss 0.025055356323719025\n",
      "Step 2800: Train Loss 0.047477059066295624 | Best Val Loss 0.025055356323719025\n",
      "Step 2900: Train Loss 1.5976704359054565 | Best Val Loss 0.025055356323719025\n",
      "Step 3000: Validation Loss 0.08767788112163544\n",
      "Step 3000: Train Loss 0.05830153450369835 | Best Val Loss 0.025055356323719025\n",
      "Step 3100: Train Loss 0.031871430575847626 | Best Val Loss 0.025055356323719025\n",
      "Step 3200: Train Loss 0.025091316550970078 | Best Val Loss 0.025055356323719025\n",
      "Step 3300: Train Loss 0.022725626826286316 | Best Val Loss 0.025055356323719025\n",
      "Step 3400: Train Loss 0.26548904180526733 | Best Val Loss 0.025055356323719025\n",
      "Step 3500: Train Loss 0.08297836780548096 | Best Val Loss 0.025055356323719025\n",
      "Step 3600: Train Loss 0.07806357741355896 | Best Val Loss 0.025055356323719025\n",
      "Step 3700: Train Loss 0.03531075641512871 | Best Val Loss 0.025055356323719025\n",
      "Step 3800: Train Loss 0.45609050989151 | Best Val Loss 0.025055356323719025\n",
      "Step 3900: Train Loss 0.09070727974176407 | Best Val Loss 0.025055356323719025\n",
      "Step 4000: Validation Loss 0.07235077023506165\n",
      "Step 4000: Train Loss 0.05235803872346878 | Best Val Loss 0.025055356323719025\n",
      "Step 4100: Train Loss 0.3700176477432251 | Best Val Loss 0.025055356323719025\n",
      "Step 4200: Train Loss 0.035418715327978134 | Best Val Loss 0.025055356323719025\n",
      "Step 4300: Train Loss 0.026839755475521088 | Best Val Loss 0.025055356323719025\n",
      "Step 4400: Train Loss 0.013462742790579796 | Best Val Loss 0.025055356323719025\n",
      "Step 4500: Train Loss 0.015704449266195297 | Best Val Loss 0.025055356323719025\n",
      "Step 4600: Train Loss 0.011333928443491459 | Best Val Loss 0.025055356323719025\n",
      "Step 4700: Train Loss 0.14582036435604095 | Best Val Loss 0.025055356323719025\n",
      "Step 4800: Train Loss 17.149166107177734 | Best Val Loss 0.025055356323719025\n",
      "Step 4900: Train Loss 0.027706719934940338 | Best Val Loss 0.025055356323719025\n",
      "Step 5000: Validation Loss 0.035703226923942566\n",
      "Step 5000: Train Loss 0.015089752152562141 | Best Val Loss 0.025055356323719025\n",
      "Step 5100: Train Loss 0.016822965815663338 | Best Val Loss 0.025055356323719025\n",
      "Step 5200: Train Loss 0.01267938781529665 | Best Val Loss 0.025055356323719025\n",
      "Step 5300: Train Loss 0.007669130805879831 | Best Val Loss 0.025055356323719025\n",
      "Step 5400: Train Loss 0.0931553840637207 | Best Val Loss 0.025055356323719025\n",
      "Step 5500: Train Loss 0.02600264362990856 | Best Val Loss 0.025055356323719025\n",
      "Step 5600: Train Loss 0.017586838454008102 | Best Val Loss 0.025055356323719025\n",
      "Step 5700: Train Loss 0.05375156179070473 | Best Val Loss 0.025055356323719025\n",
      "Step 5800: Train Loss 0.012231707572937012 | Best Val Loss 0.025055356323719025\n",
      "Step 5900: Train Loss 0.0469624400138855 | Best Val Loss 0.025055356323719025\n",
      "Step 6000: Validation Loss 0.038396578282117844\n",
      "Step 6000: Train Loss 0.021727129817008972 | Best Val Loss 0.025055356323719025\n",
      "Step 6100: Train Loss 0.6986973881721497 | Best Val Loss 0.025055356323719025\n",
      "Step 6200: Train Loss 0.07248666882514954 | Best Val Loss 0.025055356323719025\n",
      "Step 6300: Train Loss 0.032183971256017685 | Best Val Loss 0.025055356323719025\n",
      "Step 6400: Train Loss 0.024402834475040436 | Best Val Loss 0.025055356323719025\n",
      "Step 6500: Train Loss 0.01551427599042654 | Best Val Loss 0.025055356323719025\n",
      "Step 6600: Train Loss 0.2203119695186615 | Best Val Loss 0.025055356323719025\n",
      "Step 6700: Train Loss 0.03903644531965256 | Best Val Loss 0.025055356323719025\n",
      "Step 6800: Train Loss 0.0319092683494091 | Best Val Loss 0.025055356323719025\n",
      "Step 6900: Train Loss 0.017371390014886856 | Best Val Loss 0.025055356323719025\n",
      "Step 7000: Validation Loss 0.202022522687912\n",
      "Step 7000: Train Loss 0.19406768679618835 | Best Val Loss 0.025055356323719025\n",
      "Step 7100: Train Loss 0.03582232445478439 | Best Val Loss 0.025055356323719025\n",
      "Step 7200: Train Loss 0.022416094318032265 | Best Val Loss 0.025055356323719025\n",
      "Step 7300: Train Loss 0.020924922078847885 | Best Val Loss 0.025055356323719025\n",
      "Step 7400: Train Loss 0.022286083549261093 | Best Val Loss 0.025055356323719025\n",
      "Step 7500: Train Loss 0.012246929109096527 | Best Val Loss 0.025055356323719025\n",
      "Step 7600: Train Loss 0.012388947419822216 | Best Val Loss 0.025055356323719025\n",
      "Step 7700: Train Loss 0.011369070038199425 | Best Val Loss 0.025055356323719025\n",
      "Step 7800: Train Loss 0.009036998264491558 | Best Val Loss 0.025055356323719025\n",
      "Step 7900: Train Loss 0.04494428634643555 | Best Val Loss 0.025055356323719025\n",
      "Step 8000: Validation Loss 0.03471964970231056\n",
      "Step 8000: Train Loss 0.012157706543803215 | Best Val Loss 0.025055356323719025\n",
      "Step 8100: Train Loss 0.019587257876992226 | Best Val Loss 0.025055356323719025\n",
      "Step 8200: Train Loss 0.5510162115097046 | Best Val Loss 0.025055356323719025\n",
      "Step 8300: Train Loss 0.10840906202793121 | Best Val Loss 0.025055356323719025\n",
      "Step 8400: Train Loss 0.020169368013739586 | Best Val Loss 0.025055356323719025\n",
      "Step 8500: Train Loss 0.014866922050714493 | Best Val Loss 0.025055356323719025\n",
      "Step 8600: Train Loss 0.012820886448025703 | Best Val Loss 0.025055356323719025\n",
      "Step 8700: Train Loss 0.011108700186014175 | Best Val Loss 0.025055356323719025\n",
      "Step 8800: Train Loss 0.21504724025726318 | Best Val Loss 0.025055356323719025\n",
      "Step 8900: Train Loss 0.011391634121537209 | Best Val Loss 0.025055356323719025\n",
      "Step 9000: Validation Loss 0.04225977137684822\n",
      "Step 9000: Train Loss 0.011989718303084373 | Best Val Loss 0.025055356323719025\n",
      "Step 9100: Train Loss 0.010281173512339592 | Best Val Loss 0.025055356323719025\n",
      "Step 9200: Train Loss 0.029139354825019836 | Best Val Loss 0.025055356323719025\n",
      "Step 9300: Train Loss 0.013968799263238907 | Best Val Loss 0.025055356323719025\n",
      "Step 9400: Train Loss 0.012347666546702385 | Best Val Loss 0.025055356323719025\n",
      "Step 9500: Train Loss 0.014980507083237171 | Best Val Loss 0.025055356323719025\n",
      "Step 9600: Train Loss 0.007872012443840504 | Best Val Loss 0.025055356323719025\n",
      "Step 9700: Train Loss 0.006975729018449783 | Best Val Loss 0.025055356323719025\n",
      "Step 9800: Train Loss 0.016684766858816147 | Best Val Loss 0.025055356323719025\n",
      "Step 9900: Train Loss 0.009697954170405865 | Best Val Loss 0.025055356323719025\n",
      "Step 10000: Validation Loss 0.02944488264620304\n",
      "Step 10000: Train Loss 0.006424586288630962 | Best Val Loss 0.025055356323719025\n",
      "Step 10100: Train Loss 0.0301508828997612 | Best Val Loss 0.025055356323719025\n",
      "Step 10200: Train Loss 0.0109489094465971 | Best Val Loss 0.025055356323719025\n",
      "Step 10300: Train Loss 0.01032988354563713 | Best Val Loss 0.025055356323719025\n",
      "Step 10400: Train Loss 0.006708200555294752 | Best Val Loss 0.025055356323719025\n",
      "Step 10500: Train Loss 0.008718583732843399 | Best Val Loss 0.025055356323719025\n",
      "Step 10600: Train Loss 0.006821125745773315 | Best Val Loss 0.025055356323719025\n",
      "Step 10700: Train Loss 0.042027805000543594 | Best Val Loss 0.025055356323719025\n",
      "Step 10800: Train Loss 0.02463637664914131 | Best Val Loss 0.025055356323719025\n",
      "Step 10900: Train Loss 0.008259481750428677 | Best Val Loss 0.025055356323719025\n",
      "Step 11000: Validation Loss 0.025002142414450645\n",
      "Step 11000: Train Loss 0.005852373316884041 | Best Val Loss 0.025002142414450645\n",
      "Step 11100: Train Loss 0.2512432932853699 | Best Val Loss 0.025002142414450645\n",
      "Step 11200: Train Loss 0.018016107380390167 | Best Val Loss 0.025002142414450645\n",
      "Step 11300: Train Loss 0.008551409468054771 | Best Val Loss 0.025002142414450645\n",
      "Step 11400: Train Loss 0.007514341734349728 | Best Val Loss 0.025002142414450645\n",
      "Step 11500: Train Loss 0.023896511644124985 | Best Val Loss 0.025002142414450645\n",
      "Step 11600: Train Loss 0.03717966750264168 | Best Val Loss 0.025002142414450645\n",
      "Step 11700: Train Loss 0.011952325701713562 | Best Val Loss 0.025002142414450645\n",
      "Step 11800: Train Loss 0.011869573965668678 | Best Val Loss 0.025002142414450645\n",
      "Step 11900: Train Loss 0.04027663543820381 | Best Val Loss 0.025002142414450645\n",
      "Step 12000: Validation Loss 0.3537745475769043\n",
      "Step 12000: Train Loss 0.34486842155456543 | Best Val Loss 0.025002142414450645\n",
      "Step 12100: Train Loss 0.0398632250726223 | Best Val Loss 0.025002142414450645\n",
      "Step 12200: Train Loss 0.016467168927192688 | Best Val Loss 0.025002142414450645\n",
      "Step 12300: Train Loss 0.014877818524837494 | Best Val Loss 0.025002142414450645\n",
      "Step 12400: Train Loss 0.01268710009753704 | Best Val Loss 0.025002142414450645\n",
      "Step 12500: Train Loss 0.021257122978568077 | Best Val Loss 0.025002142414450645\n",
      "Step 12600: Train Loss 0.013815408572554588 | Best Val Loss 0.025002142414450645\n",
      "Step 12700: Train Loss 0.02528364025056362 | Best Val Loss 0.025002142414450645\n",
      "Step 12800: Train Loss 0.3532761335372925 | Best Val Loss 0.025002142414450645\n",
      "Step 12900: Train Loss 0.6183350086212158 | Best Val Loss 0.025002142414450645\n",
      "Step 13000: Validation Loss 0.08982748538255692\n",
      "Step 13000: Train Loss 0.06375676393508911 | Best Val Loss 0.025002142414450645\n",
      "Step 13100: Train Loss 0.014648610725998878 | Best Val Loss 0.025002142414450645\n",
      "Step 13200: Train Loss 0.010879840701818466 | Best Val Loss 0.025002142414450645\n",
      "Step 13300: Train Loss 0.005743090994656086 | Best Val Loss 0.025002142414450645\n",
      "Step 13400: Train Loss 0.009109382517635822 | Best Val Loss 0.025002142414450645\n",
      "Step 13500: Train Loss 0.005526430439203978 | Best Val Loss 0.025002142414450645\n",
      "Step 13600: Train Loss 0.008316975086927414 | Best Val Loss 0.025002142414450645\n",
      "Step 13700: Train Loss 0.10519947856664658 | Best Val Loss 0.025002142414450645\n",
      "Step 13800: Train Loss 0.01634776033461094 | Best Val Loss 0.025002142414450645\n",
      "Step 13900: Train Loss 0.045023977756500244 | Best Val Loss 0.025002142414450645\n",
      "Step 14000: Validation Loss 0.031173525378108025\n",
      "Step 14000: Train Loss 0.017402304336428642 | Best Val Loss 0.025002142414450645\n",
      "Step 14100: Train Loss 0.13560894131660461 | Best Val Loss 0.025002142414450645\n",
      "Step 14200: Train Loss 0.10467210412025452 | Best Val Loss 0.025002142414450645\n",
      "Step 14300: Train Loss 0.039323292672634125 | Best Val Loss 0.025002142414450645\n",
      "Step 14400: Train Loss 0.02855575643479824 | Best Val Loss 0.025002142414450645\n",
      "Step 14500: Train Loss 0.040678489953279495 | Best Val Loss 0.025002142414450645\n",
      "Step 14600: Train Loss 0.013737000524997711 | Best Val Loss 0.025002142414450645\n",
      "Step 14700: Train Loss 0.019910402595996857 | Best Val Loss 0.025002142414450645\n",
      "Step 14800: Train Loss 0.012333463877439499 | Best Val Loss 0.025002142414450645\n",
      "Step 14900: Train Loss 0.30208754539489746 | Best Val Loss 0.025002142414450645\n",
      "Step 15000: Validation Loss 0.05550718680024147\n",
      "Step 15000: Train Loss 0.046256326138973236 | Best Val Loss 0.025002142414450645\n",
      "Step 15100: Train Loss 0.029257891699671745 | Best Val Loss 0.025002142414450645\n",
      "Step 15200: Train Loss 0.6922620534896851 | Best Val Loss 0.025002142414450645\n",
      "Step 15300: Train Loss 4.209918022155762 | Best Val Loss 0.025002142414450645\n",
      "Step 15400: Train Loss 2.1345582008361816 | Best Val Loss 0.025002142414450645\n",
      "Step 15500: Train Loss 0.06937675923109055 | Best Val Loss 0.025002142414450645\n",
      "Step 15600: Train Loss 0.05921822041273117 | Best Val Loss 0.025002142414450645\n",
      "Step 15700: Train Loss 0.02426471933722496 | Best Val Loss 0.025002142414450645\n",
      "Step 15800: Train Loss 0.022430233657360077 | Best Val Loss 0.025002142414450645\n",
      "Step 15900: Train Loss 0.018361739814281464 | Best Val Loss 0.025002142414450645\n",
      "Step 16000: Validation Loss 0.037180036306381226\n",
      "Step 16000: Train Loss 0.020204460248351097 | Best Val Loss 0.025002142414450645\n",
      "Step 16100: Train Loss 0.8481739163398743 | Best Val Loss 0.025002142414450645\n",
      "Step 16200: Train Loss 0.03848784789443016 | Best Val Loss 0.025002142414450645\n",
      "Step 16300: Train Loss 0.020596589893102646 | Best Val Loss 0.025002142414450645\n",
      "Step 16400: Train Loss 0.01419394463300705 | Best Val Loss 0.025002142414450645\n",
      "Step 16500: Train Loss 0.008861011825501919 | Best Val Loss 0.025002142414450645\n",
      "Step 16600: Train Loss 0.007067009806632996 | Best Val Loss 0.025002142414450645\n",
      "Step 16700: Train Loss 0.006070387549698353 | Best Val Loss 0.025002142414450645\n",
      "Step 16800: Train Loss 0.005109353922307491 | Best Val Loss 0.025002142414450645\n",
      "Step 16900: Train Loss 0.02453288435935974 | Best Val Loss 0.025002142414450645\n",
      "Step 17000: Validation Loss 0.021282896399497986\n",
      "Step 17000: Train Loss 0.009256432764232159 | Best Val Loss 0.021282896399497986\n",
      "Step 17100: Train Loss 0.04816751182079315 | Best Val Loss 0.021282896399497986\n",
      "Step 17200: Train Loss 0.0678006038069725 | Best Val Loss 0.021282896399497986\n",
      "Step 17300: Train Loss 0.017188796773552895 | Best Val Loss 0.021282896399497986\n",
      "Step 17400: Train Loss 0.013429845683276653 | Best Val Loss 0.021282896399497986\n",
      "Step 17500: Train Loss 9.367643356323242 | Best Val Loss 0.021282896399497986\n",
      "Step 17600: Train Loss 0.08017873764038086 | Best Val Loss 0.021282896399497986\n",
      "Step 17700: Train Loss 0.058657821267843246 | Best Val Loss 0.021282896399497986\n",
      "Step 17800: Train Loss 0.06938314437866211 | Best Val Loss 0.021282896399497986\n",
      "Step 17900: Train Loss 0.08270935714244843 | Best Val Loss 0.021282896399497986\n",
      "Step 18000: Validation Loss 0.03269966319203377\n",
      "Step 18000: Train Loss 0.008935827761888504 | Best Val Loss 0.021282896399497986\n",
      "Step 18100: Train Loss 0.009184490889310837 | Best Val Loss 0.021282896399497986\n",
      "Step 18200: Train Loss 0.009723635390400887 | Best Val Loss 0.021282896399497986\n",
      "Step 18300: Train Loss 0.006538393907248974 | Best Val Loss 0.021282896399497986\n",
      "Step 18400: Train Loss 0.005709312856197357 | Best Val Loss 0.021282896399497986\n",
      "Step 18500: Train Loss 0.09983888268470764 | Best Val Loss 0.021282896399497986\n",
      "Step 18600: Train Loss 0.021607084199786186 | Best Val Loss 0.021282896399497986\n",
      "Step 18700: Train Loss 0.014499553479254246 | Best Val Loss 0.021282896399497986\n",
      "Step 18800: Train Loss 0.012177947908639908 | Best Val Loss 0.021282896399497986\n",
      "Step 18900: Train Loss 0.06584256142377853 | Best Val Loss 0.021282896399497986\n",
      "Step 19000: Validation Loss 0.04180591553449631\n",
      "Step 19000: Train Loss 0.015330057591199875 | Best Val Loss 0.021282896399497986\n",
      "Step 19100: Train Loss 0.015511589124798775 | Best Val Loss 0.021282896399497986\n",
      "Step 19200: Train Loss 0.07817086577415466 | Best Val Loss 0.021282896399497986\n",
      "Step 19300: Train Loss 0.32234740257263184 | Best Val Loss 0.021282896399497986\n",
      "Step 19400: Train Loss 0.10642540454864502 | Best Val Loss 0.021282896399497986\n",
      "Step 19500: Train Loss 0.042522624135017395 | Best Val Loss 0.021282896399497986\n",
      "Step 19600: Train Loss 0.027884747833013535 | Best Val Loss 0.021282896399497986\n",
      "Step 19700: Train Loss 0.016270622611045837 | Best Val Loss 0.021282896399497986\n",
      "Step 19800: Train Loss 1.2103140354156494 | Best Val Loss 0.021282896399497986\n",
      "Step 19900: Train Loss 0.05645360052585602 | Best Val Loss 0.021282896399497986\n",
      "Step 20000: Validation Loss 0.0458003506064415\n",
      "Step 20000: Train Loss 0.03040136769413948 | Best Val Loss 0.021282896399497986\n",
      "Step 20100: Train Loss 0.06610120832920074 | Best Val Loss 0.021282896399497986\n",
      "Step 20200: Train Loss 0.03920825570821762 | Best Val Loss 0.021282896399497986\n",
      "Step 20300: Train Loss 0.0913238376379013 | Best Val Loss 0.021282896399497986\n",
      "Step 20400: Train Loss 0.22885629534721375 | Best Val Loss 0.021282896399497986\n",
      "Step 20500: Train Loss 4.548690319061279 | Best Val Loss 0.021282896399497986\n",
      "Step 20600: Train Loss 0.22897271811962128 | Best Val Loss 0.021282896399497986\n",
      "Step 20700: Train Loss 0.09974177181720734 | Best Val Loss 0.021282896399497986\n",
      "Step 20800: Train Loss 3.4140453338623047 | Best Val Loss 0.021282896399497986\n",
      "Step 20900: Train Loss 0.07498446851968765 | Best Val Loss 0.021282896399497986\n",
      "Step 21000: Validation Loss 0.06685324758291245\n",
      "Step 21000: Train Loss 0.033046893775463104 | Best Val Loss 0.021282896399497986\n",
      "Step 21100: Train Loss 0.01691976562142372 | Best Val Loss 0.021282896399497986\n",
      "Step 21200: Train Loss 0.23472806811332703 | Best Val Loss 0.021282896399497986\n",
      "Step 21300: Train Loss 0.06378281861543655 | Best Val Loss 0.021282896399497986\n",
      "Step 21400: Train Loss 0.03047063946723938 | Best Val Loss 0.021282896399497986\n",
      "Step 21500: Train Loss 0.014955919235944748 | Best Val Loss 0.021282896399497986\n",
      "Step 21600: Train Loss 0.4037479758262634 | Best Val Loss 0.021282896399497986\n",
      "Step 21700: Train Loss 0.038028784096241 | Best Val Loss 0.021282896399497986\n",
      "Step 21800: Train Loss 0.020229771733283997 | Best Val Loss 0.021282896399497986\n",
      "Step 21900: Train Loss 0.013192888349294662 | Best Val Loss 0.021282896399497986\n",
      "Step 22000: Validation Loss 0.044423989951610565\n",
      "Step 22000: Train Loss 0.013899717479944229 | Best Val Loss 0.021282896399497986\n",
      "Step 22100: Train Loss 0.011644593439996243 | Best Val Loss 0.021282896399497986\n",
      "Step 22200: Train Loss 0.011613482609391212 | Best Val Loss 0.021282896399497986\n",
      "Step 22300: Train Loss 0.8398422598838806 | Best Val Loss 0.021282896399497986\n",
      "Step 22400: Train Loss 0.15759235620498657 | Best Val Loss 0.021282896399497986\n",
      "Step 22500: Train Loss 7.712671756744385 | Best Val Loss 0.021282896399497986\n",
      "Step 22600: Train Loss 0.041576169431209564 | Best Val Loss 0.021282896399497986\n",
      "Step 22700: Train Loss 0.04147784411907196 | Best Val Loss 0.021282896399497986\n",
      "Step 22800: Train Loss 0.03605297580361366 | Best Val Loss 0.021282896399497986\n",
      "Step 22900: Train Loss 0.020738529041409492 | Best Val Loss 0.021282896399497986\n",
      "Step 23000: Validation Loss 0.03256933391094208\n",
      "Step 23000: Train Loss 0.022953789681196213 | Best Val Loss 0.021282896399497986\n",
      "Step 23100: Train Loss 0.017982345074415207 | Best Val Loss 0.021282896399497986\n",
      "Step 23200: Train Loss 0.012695002369582653 | Best Val Loss 0.021282896399497986\n",
      "Step 23300: Train Loss 0.008851995691657066 | Best Val Loss 0.021282896399497986\n",
      "Step 23400: Train Loss 0.019379928708076477 | Best Val Loss 0.021282896399497986\n",
      "Step 23500: Train Loss 0.023213408887386322 | Best Val Loss 0.021282896399497986\n",
      "Step 23600: Train Loss 0.015463890507817268 | Best Val Loss 0.021282896399497986\n",
      "Step 23700: Train Loss 0.033268895000219345 | Best Val Loss 0.021282896399497986\n",
      "Step 23800: Train Loss 0.009285885840654373 | Best Val Loss 0.021282896399497986\n",
      "Step 23900: Train Loss 0.010241301730275154 | Best Val Loss 0.021282896399497986\n",
      "Step 24000: Validation Loss 0.050611112266778946\n",
      "Step 24000: Train Loss 0.03175722062587738 | Best Val Loss 0.021282896399497986\n",
      "Step 24100: Train Loss 0.019271569326519966 | Best Val Loss 0.021282896399497986\n",
      "Step 24200: Train Loss 0.01058219000697136 | Best Val Loss 0.021282896399497986\n",
      "Step 24300: Train Loss 0.011288016103208065 | Best Val Loss 0.021282896399497986\n",
      "Step 24400: Train Loss 0.010810116305947304 | Best Val Loss 0.021282896399497986\n",
      "Step 24500: Train Loss 0.009979065507650375 | Best Val Loss 0.021282896399497986\n",
      "Step 24600: Train Loss 0.043289124965667725 | Best Val Loss 0.021282896399497986\n",
      "Step 24700: Train Loss 0.010771453380584717 | Best Val Loss 0.021282896399497986\n",
      "Step 24800: Train Loss 0.01050817221403122 | Best Val Loss 0.021282896399497986\n",
      "Step 24900: Train Loss 0.008890284225344658 | Best Val Loss 0.021282896399497986\n",
      "Step 25000: Validation Loss 0.023822925984859467\n",
      "Step 25000: Train Loss 0.008541116490960121 | Best Val Loss 0.021282896399497986\n",
      "Step 25100: Train Loss 0.024291884154081345 | Best Val Loss 0.021282896399497986\n",
      "Step 25200: Train Loss 0.016887329518795013 | Best Val Loss 0.021282896399497986\n",
      "Step 25300: Train Loss 0.01903552934527397 | Best Val Loss 0.021282896399497986\n",
      "Step 25400: Train Loss 0.018208026885986328 | Best Val Loss 0.021282896399497986\n",
      "Step 25500: Train Loss 0.011685810051858425 | Best Val Loss 0.021282896399497986\n",
      "Step 25600: Train Loss 0.012101259082555771 | Best Val Loss 0.021282896399497986\n",
      "Step 25700: Train Loss 0.00899410992860794 | Best Val Loss 0.021282896399497986\n",
      "Step 25800: Train Loss 0.0772874504327774 | Best Val Loss 0.021282896399497986\n",
      "Step 25900: Train Loss 0.027119360864162445 | Best Val Loss 0.021282896399497986\n",
      "Step 26000: Validation Loss 0.04693661257624626\n",
      "Step 26000: Train Loss 0.03395504876971245 | Best Val Loss 0.021282896399497986\n",
      "Step 26100: Train Loss 0.01754840835928917 | Best Val Loss 0.021282896399497986\n",
      "Step 26200: Train Loss 0.01926811970770359 | Best Val Loss 0.021282896399497986\n",
      "Step 26300: Train Loss 0.011149771511554718 | Best Val Loss 0.021282896399497986\n",
      "Step 26400: Train Loss 0.02794448658823967 | Best Val Loss 0.021282896399497986\n",
      "Step 26500: Train Loss 0.012195460498332977 | Best Val Loss 0.021282896399497986\n",
      "Step 26600: Train Loss 0.011085743084549904 | Best Val Loss 0.021282896399497986\n",
      "Step 26700: Train Loss 0.007278574630618095 | Best Val Loss 0.021282896399497986\n",
      "Step 26800: Train Loss 0.006286844611167908 | Best Val Loss 0.021282896399497986\n",
      "Step 26900: Train Loss 0.006485156714916229 | Best Val Loss 0.021282896399497986\n",
      "Step 27000: Validation Loss 0.022344252094626427\n",
      "Step 27000: Train Loss 0.005735907703638077 | Best Val Loss 0.021282896399497986\n",
      "Step 27100: Train Loss 0.006016368046402931 | Best Val Loss 0.021282896399497986\n",
      "Step 27200: Train Loss 0.007565597537904978 | Best Val Loss 0.021282896399497986\n",
      "Step 27300: Train Loss 0.006997975055128336 | Best Val Loss 0.021282896399497986\n",
      "Step 27400: Train Loss 0.006466131191700697 | Best Val Loss 0.021282896399497986\n",
      "Step 27500: Train Loss 0.01007632166147232 | Best Val Loss 0.021282896399497986\n",
      "Step 27600: Train Loss 0.006562720984220505 | Best Val Loss 0.021282896399497986\n",
      "Step 27700: Train Loss 0.026996038854122162 | Best Val Loss 0.021282896399497986\n",
      "Step 27800: Train Loss 2.607022762298584 | Best Val Loss 0.021282896399497986\n",
      "Step 27900: Train Loss 0.01796266809105873 | Best Val Loss 0.021282896399497986\n",
      "Step 28000: Validation Loss 0.02401319518685341\n",
      "Step 28000: Train Loss 0.010147430002689362 | Best Val Loss 0.021282896399497986\n",
      "Step 28100: Train Loss 1.0010294914245605 | Best Val Loss 0.021282896399497986\n",
      "Step 28200: Train Loss 0.007386910729110241 | Best Val Loss 0.021282896399497986\n",
      "Step 28300: Train Loss 0.0084442850202322 | Best Val Loss 0.021282896399497986\n",
      "Step 28400: Train Loss 0.022786611691117287 | Best Val Loss 0.021282896399497986\n",
      "Step 28500: Train Loss 0.0068663135170936584 | Best Val Loss 0.021282896399497986\n",
      "Step 28600: Train Loss 0.006352458149194717 | Best Val Loss 0.021282896399497986\n",
      "Step 28700: Train Loss 0.005255402065813541 | Best Val Loss 0.021282896399497986\n",
      "Step 28800: Train Loss 0.009457077831029892 | Best Val Loss 0.021282896399497986\n",
      "Step 28900: Train Loss 0.010434887371957302 | Best Val Loss 0.021282896399497986\n",
      "Step 29000: Validation Loss 0.03195264935493469\n",
      "Step 29000: Train Loss 0.01018926128745079 | Best Val Loss 0.021282896399497986\n",
      "Step 29100: Train Loss 11.710649490356445 | Best Val Loss 0.021282896399497986\n",
      "Step 29200: Train Loss 0.02266034483909607 | Best Val Loss 0.021282896399497986\n",
      "Step 29300: Train Loss 0.1553494930267334 | Best Val Loss 0.021282896399497986\n",
      "Step 29400: Train Loss 0.025693435221910477 | Best Val Loss 0.021282896399497986\n",
      "Step 29500: Train Loss 0.01514781080186367 | Best Val Loss 0.021282896399497986\n",
      "Step 29600: Train Loss 0.021478574723005295 | Best Val Loss 0.021282896399497986\n",
      "Step 29700: Train Loss 0.02266298048198223 | Best Val Loss 0.021282896399497986\n",
      "Step 29800: Train Loss 0.01246732473373413 | Best Val Loss 0.021282896399497986\n",
      "Step 29900: Train Loss 0.013801113702356815 | Best Val Loss 0.021282896399497986\n",
      "Step 30000: Validation Loss 0.048835694789886475\n",
      "Step 30000: Train Loss 0.03240694850683212 | Best Val Loss 0.021282896399497986\n",
      "Step 30100: Train Loss 0.0871453732252121 | Best Val Loss 0.021282896399497986\n",
      "Step 30200: Train Loss 0.013320374302566051 | Best Val Loss 0.021282896399497986\n",
      "Step 30300: Train Loss 0.031320326030254364 | Best Val Loss 0.021282896399497986\n",
      "Step 30400: Train Loss 0.25224050879478455 | Best Val Loss 0.021282896399497986\n",
      "Step 30500: Train Loss 0.23932164907455444 | Best Val Loss 0.021282896399497986\n",
      "Step 30600: Train Loss 1.4823811054229736 | Best Val Loss 0.021282896399497986\n",
      "Step 30700: Train Loss 0.07566404342651367 | Best Val Loss 0.021282896399497986\n",
      "Step 30800: Train Loss 0.028098227456212044 | Best Val Loss 0.021282896399497986\n",
      "Step 30900: Train Loss 0.019109955057501793 | Best Val Loss 0.021282896399497986\n",
      "Step 31000: Validation Loss 0.037042345851659775\n",
      "Step 31000: Train Loss 0.02454800345003605 | Best Val Loss 0.021282896399497986\n",
      "Step 31100: Train Loss 0.03610728681087494 | Best Val Loss 0.021282896399497986\n",
      "Step 31200: Train Loss 0.07592413574457169 | Best Val Loss 0.021282896399497986\n",
      "Step 31300: Train Loss 0.021622033789753914 | Best Val Loss 0.021282896399497986\n",
      "Step 31400: Train Loss 0.31973177194595337 | Best Val Loss 0.021282896399497986\n",
      "Step 31500: Train Loss 0.0570700503885746 | Best Val Loss 0.021282896399497986\n",
      "Step 31600: Train Loss 5.32921838760376 | Best Val Loss 0.021282896399497986\n",
      "Step 31700: Train Loss 0.017639532685279846 | Best Val Loss 0.021282896399497986\n",
      "Step 31800: Train Loss 0.01649564877152443 | Best Val Loss 0.021282896399497986\n",
      "Step 31900: Train Loss 0.01430978998541832 | Best Val Loss 0.021282896399497986\n",
      "Step 32000: Validation Loss 0.255174458026886\n",
      "Step 32000: Train Loss 0.12951505184173584 | Best Val Loss 0.021282896399497986\n",
      "Step 32100: Train Loss 0.08271211385726929 | Best Val Loss 0.021282896399497986\n",
      "Step 32200: Train Loss 0.04074252024292946 | Best Val Loss 0.021282896399497986\n",
      "Step 32300: Train Loss 0.028916478157043457 | Best Val Loss 0.021282896399497986\n",
      "Step 32400: Train Loss 0.05268368124961853 | Best Val Loss 0.021282896399497986\n",
      "Step 32500: Train Loss 0.028178203850984573 | Best Val Loss 0.021282896399497986\n",
      "Step 32600: Train Loss 0.03325043246150017 | Best Val Loss 0.021282896399497986\n",
      "Step 32700: Train Loss 0.06438755989074707 | Best Val Loss 0.021282896399497986\n",
      "Step 32800: Train Loss 0.02621525339782238 | Best Val Loss 0.021282896399497986\n",
      "Step 32900: Train Loss 0.020827623084187508 | Best Val Loss 0.021282896399497986\n",
      "Step 33000: Validation Loss 0.055591732263565063\n",
      "Step 33000: Train Loss 0.028038766235113144 | Best Val Loss 0.021282896399497986\n",
      "Step 33100: Train Loss 0.028344519436359406 | Best Val Loss 0.021282896399497986\n",
      "Step 33200: Train Loss 0.012925984337925911 | Best Val Loss 0.021282896399497986\n",
      "Step 33300: Train Loss 0.013921458274126053 | Best Val Loss 0.021282896399497986\n",
      "Step 33400: Train Loss 0.2625945508480072 | Best Val Loss 0.021282896399497986\n",
      "Step 33500: Train Loss 0.022810697555541992 | Best Val Loss 0.021282896399497986\n",
      "Step 33600: Train Loss 0.0751284807920456 | Best Val Loss 0.021282896399497986\n",
      "Step 33700: Train Loss 0.13919752836227417 | Best Val Loss 0.021282896399497986\n",
      "Step 33800: Train Loss 10.609092712402344 | Best Val Loss 0.021282896399497986\n",
      "Step 33900: Train Loss 0.26051005721092224 | Best Val Loss 0.021282896399497986\n",
      "Step 34000: Validation Loss 0.4037642478942871\n",
      "Step 34000: Train Loss 0.2407374233007431 | Best Val Loss 0.021282896399497986\n",
      "Step 34100: Train Loss 0.08316149562597275 | Best Val Loss 0.021282896399497986\n",
      "Step 34200: Train Loss 0.05231041461229324 | Best Val Loss 0.021282896399497986\n",
      "Step 34300: Train Loss 0.028140682727098465 | Best Val Loss 0.021282896399497986\n",
      "Step 34400: Train Loss 0.028408240526914597 | Best Val Loss 0.021282896399497986\n",
      "Step 34500: Train Loss 0.017515011131763458 | Best Val Loss 0.021282896399497986\n",
      "Step 34600: Train Loss 0.0210625808686018 | Best Val Loss 0.021282896399497986\n",
      "Step 34700: Train Loss 1.0890278816223145 | Best Val Loss 0.021282896399497986\n",
      "Step 34800: Train Loss 0.09476268291473389 | Best Val Loss 0.021282896399497986\n",
      "Step 34900: Train Loss 0.06160438433289528 | Best Val Loss 0.021282896399497986\n",
      "Step 35000: Validation Loss 0.8729119300842285\n",
      "Step 35000: Train Loss 0.6249163150787354 | Best Val Loss 0.021282896399497986\n",
      "Step 35100: Train Loss 0.12017011642456055 | Best Val Loss 0.021282896399497986\n",
      "Step 35200: Train Loss 0.04946473240852356 | Best Val Loss 0.021282896399497986\n",
      "Step 35300: Train Loss 0.34462136030197144 | Best Val Loss 0.021282896399497986\n",
      "Step 35400: Train Loss 0.13637760281562805 | Best Val Loss 0.021282896399497986\n",
      "Step 35500: Train Loss 0.05751548707485199 | Best Val Loss 0.021282896399497986\n",
      "Step 35600: Train Loss 0.04841490834951401 | Best Val Loss 0.021282896399497986\n",
      "Step 35700: Train Loss 0.024005070328712463 | Best Val Loss 0.021282896399497986\n",
      "Step 35800: Train Loss 0.015679191797971725 | Best Val Loss 0.021282896399497986\n",
      "Step 35900: Train Loss 0.014643372967839241 | Best Val Loss 0.021282896399497986\n",
      "Step 36000: Validation Loss 0.030621683225035667\n",
      "Step 36000: Train Loss 0.012735020369291306 | Best Val Loss 0.021282896399497986\n",
      "Step 36100: Train Loss 0.011974598281085491 | Best Val Loss 0.021282896399497986\n",
      "Step 36200: Train Loss 0.02547628805041313 | Best Val Loss 0.021282896399497986\n",
      "Step 36300: Train Loss 0.010050765238702297 | Best Val Loss 0.021282896399497986\n",
      "Step 36400: Train Loss 0.010352295823395252 | Best Val Loss 0.021282896399497986\n",
      "Step 36500: Train Loss 0.008864195086061954 | Best Val Loss 0.021282896399497986\n",
      "Step 36600: Train Loss 0.011788790114223957 | Best Val Loss 0.021282896399497986\n",
      "Step 36700: Train Loss 0.009889182634651661 | Best Val Loss 0.021282896399497986\n",
      "Step 36800: Train Loss 0.008289827033877373 | Best Val Loss 0.021282896399497986\n",
      "Step 36900: Train Loss 0.009546194225549698 | Best Val Loss 0.021282896399497986\n",
      "Step 37000: Validation Loss 0.1359660029411316\n",
      "Step 37000: Train Loss 0.09574252367019653 | Best Val Loss 0.021282896399497986\n",
      "Step 37100: Train Loss 0.028352588415145874 | Best Val Loss 0.021282896399497986\n",
      "Step 37200: Train Loss 0.027365610003471375 | Best Val Loss 0.021282896399497986\n",
      "Step 37300: Train Loss 0.021167170256376266 | Best Val Loss 0.021282896399497986\n",
      "Step 37400: Train Loss 0.016210392117500305 | Best Val Loss 0.021282896399497986\n",
      "Step 37500: Train Loss 0.08839866518974304 | Best Val Loss 0.021282896399497986\n",
      "Step 37600: Train Loss 0.023197222501039505 | Best Val Loss 0.021282896399497986\n",
      "Step 37700: Train Loss 0.016704216599464417 | Best Val Loss 0.021282896399497986\n",
      "Step 37800: Train Loss 0.01444663293659687 | Best Val Loss 0.021282896399497986\n",
      "Step 37900: Train Loss 0.014870144426822662 | Best Val Loss 0.021282896399497986\n",
      "Step 38000: Validation Loss 0.06028752401471138\n",
      "Step 38000: Train Loss 0.037416208535432816 | Best Val Loss 0.021282896399497986\n",
      "Step 38100: Train Loss 0.01809697411954403 | Best Val Loss 0.021282896399497986\n",
      "Step 38200: Train Loss 0.01707710698246956 | Best Val Loss 0.021282896399497986\n",
      "Step 38300: Train Loss 0.09259191155433655 | Best Val Loss 0.021282896399497986\n",
      "Step 38400: Train Loss 0.12980307638645172 | Best Val Loss 0.021282896399497986\n",
      "Step 38500: Train Loss 0.02965756505727768 | Best Val Loss 0.021282896399497986\n",
      "Step 38600: Train Loss 0.02148035541176796 | Best Val Loss 0.021282896399497986\n",
      "Step 38700: Train Loss 0.013550475239753723 | Best Val Loss 0.021282896399497986\n",
      "Step 38800: Train Loss 0.11725323647260666 | Best Val Loss 0.021282896399497986\n",
      "Step 38900: Train Loss 0.09356799721717834 | Best Val Loss 0.021282896399497986\n",
      "Step 39000: Validation Loss 0.16981691122055054\n",
      "Step 39000: Train Loss 0.14160968363285065 | Best Val Loss 0.021282896399497986\n",
      "Step 39100: Train Loss 0.34126877784729004 | Best Val Loss 0.021282896399497986\n",
      "Step 39200: Train Loss 0.44493719935417175 | Best Val Loss 0.021282896399497986\n",
      "Step 39300: Train Loss 0.05317825451493263 | Best Val Loss 0.021282896399497986\n",
      "Step 39400: Train Loss 0.5100094676017761 | Best Val Loss 0.021282896399497986\n",
      "Step 39500: Train Loss 0.04183146357536316 | Best Val Loss 0.021282896399497986\n",
      "Step 39600: Train Loss 0.5066026449203491 | Best Val Loss 0.021282896399497986\n",
      "Step 39700: Train Loss 0.13640189170837402 | Best Val Loss 0.021282896399497986\n",
      "Step 39800: Train Loss 0.11513420939445496 | Best Val Loss 0.021282896399497986\n",
      "Step 39900: Train Loss 0.10528865456581116 | Best Val Loss 0.021282896399497986\n",
      "Step 40000: Validation Loss 0.1437779664993286\n",
      "Step 40000: Train Loss 0.07834584265947342 | Best Val Loss 0.021282896399497986\n",
      "Step 40100: Train Loss 0.09690269827842712 | Best Val Loss 0.021282896399497986\n",
      "Step 40200: Train Loss 0.08115912973880768 | Best Val Loss 0.021282896399497986\n",
      "Step 40300: Train Loss 0.06982535123825073 | Best Val Loss 0.021282896399497986\n",
      "Step 40400: Train Loss 0.061596423387527466 | Best Val Loss 0.021282896399497986\n",
      "Step 40500: Train Loss 0.19363290071487427 | Best Val Loss 0.021282896399497986\n",
      "Step 40600: Train Loss 0.6704332232475281 | Best Val Loss 0.021282896399497986\n",
      "Step 40700: Train Loss 0.14220182597637177 | Best Val Loss 0.021282896399497986\n",
      "Step 40800: Train Loss 0.13184446096420288 | Best Val Loss 0.021282896399497986\n",
      "Step 40900: Train Loss 0.05570506304502487 | Best Val Loss 0.021282896399497986\n",
      "Step 41000: Validation Loss 0.2829206585884094\n",
      "Step 41000: Train Loss 0.19282864034175873 | Best Val Loss 0.021282896399497986\n",
      "Step 41100: Train Loss 2.4019670486450195 | Best Val Loss 0.021282896399497986\n",
      "Step 41200: Train Loss 0.19809025526046753 | Best Val Loss 0.021282896399497986\n",
      "Step 41300: Train Loss 0.06726045161485672 | Best Val Loss 0.021282896399497986\n",
      "Step 41400: Train Loss 24.779361724853516 | Best Val Loss 0.021282896399497986\n",
      "Step 41500: Train Loss 1.2297091484069824 | Best Val Loss 0.021282896399497986\n",
      "Step 41600: Train Loss 0.6068097352981567 | Best Val Loss 0.021282896399497986\n",
      "Step 41700: Train Loss 0.527174174785614 | Best Val Loss 0.021282896399497986\n",
      "Step 41800: Train Loss 0.2885512113571167 | Best Val Loss 0.021282896399497986\n",
      "Step 41900: Train Loss 0.32780393958091736 | Best Val Loss 0.021282896399497986\n",
      "Step 42000: Validation Loss 0.22318996489048004\n",
      "Step 42000: Train Loss 0.2313087433576584 | Best Val Loss 0.021282896399497986\n",
      "Step 42100: Train Loss 0.1794804185628891 | Best Val Loss 0.021282896399497986\n",
      "Step 42200: Train Loss 0.3863627314567566 | Best Val Loss 0.021282896399497986\n",
      "Step 42300: Train Loss 0.2312382310628891 | Best Val Loss 0.021282896399497986\n",
      "Step 42400: Train Loss 0.17266038060188293 | Best Val Loss 0.021282896399497986\n",
      "Step 42500: Train Loss 0.10671892762184143 | Best Val Loss 0.021282896399497986\n",
      "Step 42600: Train Loss 0.11254630982875824 | Best Val Loss 0.021282896399497986\n",
      "Step 42700: Train Loss 0.08774061501026154 | Best Val Loss 0.021282896399497986\n",
      "Step 42800: Train Loss 0.22082599997520447 | Best Val Loss 0.021282896399497986\n",
      "Step 42900: Train Loss 0.10304269194602966 | Best Val Loss 0.021282896399497986\n",
      "Step 43000: Validation Loss 0.10734031349420547\n",
      "Step 43000: Train Loss 0.10529792308807373 | Best Val Loss 0.021282896399497986\n",
      "Step 43100: Train Loss 0.13284990191459656 | Best Val Loss 0.021282896399497986\n",
      "Step 43200: Train Loss 3.3317017555236816 | Best Val Loss 0.021282896399497986\n",
      "Step 43300: Train Loss 0.7001334428787231 | Best Val Loss 0.021282896399497986\n",
      "Step 43400: Train Loss 3.4129316806793213 | Best Val Loss 0.021282896399497986\n",
      "Step 43500: Train Loss 0.6138182282447815 | Best Val Loss 0.021282896399497986\n",
      "Step 43600: Train Loss 0.3083491921424866 | Best Val Loss 0.021282896399497986\n",
      "Step 43700: Train Loss 29.295391082763672 | Best Val Loss 0.021282896399497986\n",
      "Step 43800: Train Loss 0.34710928797721863 | Best Val Loss 0.021282896399497986\n",
      "Step 43900: Train Loss 5.123844623565674 | Best Val Loss 0.021282896399497986\n",
      "Step 44000: Validation Loss 1.2705280780792236\n",
      "Step 44000: Train Loss 0.7850896120071411 | Best Val Loss 0.021282896399497986\n",
      "Step 44100: Train Loss 0.552106499671936 | Best Val Loss 0.021282896399497986\n",
      "Step 44200: Train Loss 0.3085651099681854 | Best Val Loss 0.021282896399497986\n",
      "Step 44300: Train Loss 0.23060797154903412 | Best Val Loss 0.021282896399497986\n",
      "Step 44400: Train Loss 0.2790966033935547 | Best Val Loss 0.021282896399497986\n",
      "Step 44500: Train Loss 0.15329673886299133 | Best Val Loss 0.021282896399497986\n",
      "Step 44600: Train Loss 9.53780746459961 | Best Val Loss 0.021282896399497986\n",
      "Step 44700: Train Loss 2.044358968734741 | Best Val Loss 0.021282896399497986\n",
      "Step 44800: Train Loss 2.5760083198547363 | Best Val Loss 0.021282896399497986\n",
      "Step 44900: Train Loss 1.05372953414917 | Best Val Loss 0.021282896399497986\n",
      "Step 45000: Validation Loss 0.76675945520401\n",
      "Step 45000: Train Loss 0.6549041271209717 | Best Val Loss 0.021282896399497986\n",
      "Step 45100: Train Loss 0.3789767622947693 | Best Val Loss 0.021282896399497986\n",
      "Step 45200: Train Loss 0.4696456789970398 | Best Val Loss 0.021282896399497986\n",
      "Step 45300: Train Loss 0.33232003450393677 | Best Val Loss 0.021282896399497986\n",
      "Step 45400: Train Loss 0.1944563388824463 | Best Val Loss 0.021282896399497986\n",
      "Step 45500: Train Loss 0.1268903762102127 | Best Val Loss 0.021282896399497986\n",
      "Step 45600: Train Loss 0.0894724503159523 | Best Val Loss 0.021282896399497986\n",
      "Step 45700: Train Loss 0.06466728448867798 | Best Val Loss 0.021282896399497986\n",
      "Step 45800: Train Loss 0.06235659122467041 | Best Val Loss 0.021282896399497986\n",
      "Step 45900: Train Loss 0.07046350836753845 | Best Val Loss 0.021282896399497986\n",
      "Step 46000: Validation Loss 0.06477479636669159\n",
      "Step 46000: Train Loss 0.04279261454939842 | Best Val Loss 0.021282896399497986\n",
      "Step 46100: Train Loss 0.04484965652227402 | Best Val Loss 0.021282896399497986\n",
      "Step 46200: Train Loss 0.1227836161851883 | Best Val Loss 0.021282896399497986\n",
      "Step 46300: Train Loss 0.05493296682834625 | Best Val Loss 0.021282896399497986\n",
      "Step 46400: Train Loss 0.047134045511484146 | Best Val Loss 0.021282896399497986\n",
      "Step 46500: Train Loss 0.030200419947504997 | Best Val Loss 0.021282896399497986\n",
      "Step 46600: Train Loss 0.02748868055641651 | Best Val Loss 0.021282896399497986\n",
      "Step 46700: Train Loss 0.18853574991226196 | Best Val Loss 0.021282896399497986\n",
      "Step 46800: Train Loss 0.1609455794095993 | Best Val Loss 0.021282896399497986\n",
      "Step 46900: Train Loss 0.07419814169406891 | Best Val Loss 0.021282896399497986\n",
      "Step 47000: Validation Loss 0.0918416753411293\n",
      "Step 47000: Train Loss 0.066250741481781 | Best Val Loss 0.021282896399497986\n",
      "Step 47100: Train Loss 0.04832649976015091 | Best Val Loss 0.021282896399497986\n",
      "Step 47200: Train Loss 0.06908935308456421 | Best Val Loss 0.021282896399497986\n",
      "Step 47300: Train Loss 0.04331517964601517 | Best Val Loss 0.021282896399497986\n",
      "Step 47400: Train Loss 0.1713278889656067 | Best Val Loss 0.021282896399497986\n",
      "Step 47500: Train Loss 0.2298887073993683 | Best Val Loss 0.021282896399497986\n",
      "Step 47600: Train Loss 0.07461346685886383 | Best Val Loss 0.021282896399497986\n",
      "Step 47700: Train Loss 0.07973537594079971 | Best Val Loss 0.021282896399497986\n",
      "Step 47800: Train Loss 0.06373695284128189 | Best Val Loss 0.021282896399497986\n",
      "Step 47900: Train Loss 0.049505479633808136 | Best Val Loss 0.021282896399497986\n",
      "Step 48000: Validation Loss 0.06044849008321762\n",
      "Step 48000: Train Loss 0.05638890340924263 | Best Val Loss 0.021282896399497986\n",
      "Step 48100: Train Loss 0.0417402908205986 | Best Val Loss 0.021282896399497986\n",
      "Step 48200: Train Loss 0.03888378664851189 | Best Val Loss 0.021282896399497986\n",
      "Step 48300: Train Loss 0.03129638731479645 | Best Val Loss 0.021282896399497986\n",
      "Step 48400: Train Loss 0.037307582795619965 | Best Val Loss 0.021282896399497986\n",
      "Step 48500: Train Loss 0.027039669454097748 | Best Val Loss 0.021282896399497986\n",
      "Step 48600: Train Loss 0.02959640696644783 | Best Val Loss 0.021282896399497986\n",
      "Step 48700: Train Loss 0.023503484204411507 | Best Val Loss 0.021282896399497986\n",
      "Step 48800: Train Loss 0.022260602563619614 | Best Val Loss 0.021282896399497986\n",
      "Step 48900: Train Loss 0.03112672083079815 | Best Val Loss 0.021282896399497986\n",
      "Step 49000: Validation Loss 0.0394669733941555\n",
      "Step 49000: Train Loss 0.024358587339520454 | Best Val Loss 0.021282896399497986\n",
      "Step 49100: Train Loss 0.02015036903321743 | Best Val Loss 0.021282896399497986\n",
      "Step 49200: Train Loss 0.024578655138611794 | Best Val Loss 0.021282896399497986\n",
      "Step 49300: Train Loss 0.021634582430124283 | Best Val Loss 0.021282896399497986\n",
      "Step 49400: Train Loss 0.020481102168560028 | Best Val Loss 0.021282896399497986\n",
      "Step 49500: Train Loss 0.11827842891216278 | Best Val Loss 0.021282896399497986\n",
      "Step 49600: Train Loss 0.07104966044425964 | Best Val Loss 0.021282896399497986\n",
      "Step 49700: Train Loss 0.06210215389728546 | Best Val Loss 0.021282896399497986\n",
      "Step 49800: Train Loss 0.03443356603384018 | Best Val Loss 0.021282896399497986\n",
      "Step 49900: Train Loss 0.03152468800544739 | Best Val Loss 0.021282896399497986\n",
      "Step 50000: Validation Loss 0.04226836562156677\n",
      "Step 50000: Train Loss 0.02497430518269539 | Best Val Loss 0.021282896399497986\n",
      "Step 50100: Train Loss 0.023292332887649536 | Best Val Loss 0.021282896399497986\n",
      "Step 50200: Train Loss 0.016610844060778618 | Best Val Loss 0.021282896399497986\n",
      "Step 50300: Train Loss 0.015654005110263824 | Best Val Loss 0.021282896399497986\n",
      "Step 50400: Train Loss 0.013103884644806385 | Best Val Loss 0.021282896399497986\n",
      "Step 50500: Train Loss 0.01646980084478855 | Best Val Loss 0.021282896399497986\n",
      "Step 50600: Train Loss 0.047962166368961334 | Best Val Loss 0.021282896399497986\n",
      "Step 50700: Train Loss 0.0836462527513504 | Best Val Loss 0.021282896399497986\n",
      "Step 50800: Train Loss 0.04626872390508652 | Best Val Loss 0.021282896399497986\n",
      "Step 50900: Train Loss 0.04287654906511307 | Best Val Loss 0.021282896399497986\n",
      "Step 51000: Validation Loss 0.04802137613296509\n",
      "Step 51000: Train Loss 0.031070834025740623 | Best Val Loss 0.021282896399497986\n",
      "Step 51100: Train Loss 0.023141754791140556 | Best Val Loss 0.021282896399497986\n",
      "Step 51200: Train Loss 0.019477706402540207 | Best Val Loss 0.021282896399497986\n",
      "Step 51300: Train Loss 0.015420615673065186 | Best Val Loss 0.021282896399497986\n",
      "Step 51400: Train Loss 0.014643792062997818 | Best Val Loss 0.021282896399497986\n",
      "Step 51500: Train Loss 0.013248082250356674 | Best Val Loss 0.021282896399497986\n",
      "Step 51600: Train Loss 0.012673466466367245 | Best Val Loss 0.021282896399497986\n",
      "Step 51700: Train Loss 0.013457505032420158 | Best Val Loss 0.021282896399497986\n",
      "Step 51800: Train Loss 0.013936745934188366 | Best Val Loss 0.021282896399497986\n",
      "Step 51900: Train Loss 0.009875823743641376 | Best Val Loss 0.021282896399497986\n",
      "Step 52000: Validation Loss 0.02511010505259037\n",
      "Step 52000: Train Loss 0.010875232517719269 | Best Val Loss 0.021282896399497986\n",
      "Step 52100: Train Loss 0.009299689903855324 | Best Val Loss 0.021282896399497986\n",
      "Step 52200: Train Loss 0.010709015652537346 | Best Val Loss 0.021282896399497986\n",
      "Step 52300: Train Loss 0.019355816766619682 | Best Val Loss 0.021282896399497986\n",
      "Step 52400: Train Loss 0.020053613930940628 | Best Val Loss 0.021282896399497986\n",
      "Step 52500: Train Loss 0.012803265824913979 | Best Val Loss 0.021282896399497986\n",
      "Step 52600: Train Loss 0.01685556210577488 | Best Val Loss 0.021282896399497986\n",
      "Step 52700: Train Loss 0.016867514699697495 | Best Val Loss 0.021282896399497986\n",
      "Step 52800: Train Loss 0.01053193025290966 | Best Val Loss 0.021282896399497986\n",
      "Step 52900: Train Loss 0.038885243237018585 | Best Val Loss 0.021282896399497986\n",
      "Step 53000: Validation Loss 0.11367978900671005\n",
      "Step 53000: Train Loss 0.0855456218123436 | Best Val Loss 0.021282896399497986\n",
      "Step 53100: Train Loss 0.08304468542337418 | Best Val Loss 0.021282896399497986\n",
      "Step 53200: Train Loss 0.5866535902023315 | Best Val Loss 0.021282896399497986\n",
      "Step 53300: Train Loss 0.15668803453445435 | Best Val Loss 0.021282896399497986\n",
      "Step 53400: Train Loss 0.07441433519124985 | Best Val Loss 0.021282896399497986\n",
      "Step 53500: Train Loss 0.10942410677671432 | Best Val Loss 0.021282896399497986\n",
      "Step 53600: Train Loss 0.035686761140823364 | Best Val Loss 0.021282896399497986\n",
      "Step 53700: Train Loss 0.0239423718303442 | Best Val Loss 0.021282896399497986\n",
      "Step 53800: Train Loss 0.02470964938402176 | Best Val Loss 0.021282896399497986\n",
      "Step 53900: Train Loss 0.01740812323987484 | Best Val Loss 0.021282896399497986\n",
      "Step 54000: Validation Loss 0.034612275660037994\n",
      "Step 54000: Train Loss 0.013712109997868538 | Best Val Loss 0.021282896399497986\n",
      "Step 54100: Train Loss 0.010952889919281006 | Best Val Loss 0.021282896399497986\n",
      "Step 54200: Train Loss 0.016866015270352364 | Best Val Loss 0.021282896399497986\n",
      "Step 54300: Train Loss 0.024492725729942322 | Best Val Loss 0.021282896399497986\n",
      "Step 54400: Train Loss 0.00967034325003624 | Best Val Loss 0.021282896399497986\n",
      "Step 54500: Train Loss 0.008488761261105537 | Best Val Loss 0.021282896399497986\n",
      "Step 54600: Train Loss 0.008921369910240173 | Best Val Loss 0.021282896399497986\n",
      "Step 54700: Train Loss 0.2437780201435089 | Best Val Loss 0.021282896399497986\n",
      "Step 54800: Train Loss 0.06461620330810547 | Best Val Loss 0.021282896399497986\n",
      "Step 54900: Train Loss 0.03486704081296921 | Best Val Loss 0.021282896399497986\n",
      "Step 55000: Validation Loss 0.05567927286028862\n",
      "Step 55000: Train Loss 0.019272826611995697 | Best Val Loss 0.021282896399497986\n",
      "Step 55100: Train Loss 0.06691128015518188 | Best Val Loss 0.021282896399497986\n",
      "Step 55200: Train Loss 0.017637021839618683 | Best Val Loss 0.021282896399497986\n",
      "Step 55300: Train Loss 0.1409817934036255 | Best Val Loss 0.021282896399497986\n",
      "Step 55400: Train Loss 0.027933413162827492 | Best Val Loss 0.021282896399497986\n",
      "Step 55500: Train Loss 0.022374894469976425 | Best Val Loss 0.021282896399497986\n",
      "Step 55600: Train Loss 3.576446533203125 | Best Val Loss 0.021282896399497986\n",
      "Step 55700: Train Loss 0.22113224864006042 | Best Val Loss 0.021282896399497986\n",
      "Step 55800: Train Loss 0.10714422911405563 | Best Val Loss 0.021282896399497986\n",
      "Step 55900: Train Loss 0.1026749461889267 | Best Val Loss 0.021282896399497986\n",
      "Step 56000: Validation Loss 0.07535874843597412\n",
      "Step 56000: Train Loss 0.061343930661678314 | Best Val Loss 0.021282896399497986\n",
      "Step 56100: Train Loss 0.12715435028076172 | Best Val Loss 0.021282896399497986\n",
      "Step 56200: Train Loss 0.04185256361961365 | Best Val Loss 0.021282896399497986\n",
      "Step 56300: Train Loss 0.02757979929447174 | Best Val Loss 0.021282896399497986\n",
      "Step 56400: Train Loss 0.06418504565954208 | Best Val Loss 0.021282896399497986\n",
      "Step 56500: Train Loss 0.022405970841646194 | Best Val Loss 0.021282896399497986\n",
      "Step 56600: Train Loss 0.017596136778593063 | Best Val Loss 0.021282896399497986\n",
      "Step 56700: Train Loss 0.10937869548797607 | Best Val Loss 0.021282896399497986\n",
      "Step 56800: Train Loss 0.03542790561914444 | Best Val Loss 0.021282896399497986\n",
      "Step 56900: Train Loss 0.1527126431465149 | Best Val Loss 0.021282896399497986\n",
      "Step 57000: Validation Loss 0.04205853119492531\n",
      "Step 57000: Train Loss 0.026351910084486008 | Best Val Loss 0.021282896399497986\n",
      "Step 57100: Train Loss 0.020904559642076492 | Best Val Loss 0.021282896399497986\n",
      "Step 57200: Train Loss 1.428306221961975 | Best Val Loss 0.021282896399497986\n",
      "Step 57300: Train Loss 0.18802984058856964 | Best Val Loss 0.021282896399497986\n",
      "Step 57400: Train Loss 0.06886936724185944 | Best Val Loss 0.021282896399497986\n",
      "Step 57500: Train Loss 1.6782513856887817 | Best Val Loss 0.021282896399497986\n",
      "Step 57600: Train Loss 0.38293319940567017 | Best Val Loss 0.021282896399497986\n",
      "Step 57700: Train Loss 0.060670316219329834 | Best Val Loss 0.021282896399497986\n",
      "Step 57800: Train Loss 0.38740408420562744 | Best Val Loss 0.021282896399497986\n",
      "Step 57900: Train Loss 0.05156581103801727 | Best Val Loss 0.021282896399497986\n",
      "Step 58000: Validation Loss 0.04343973472714424\n",
      "Step 58000: Train Loss 0.0356239378452301 | Best Val Loss 0.021282896399497986\n",
      "Step 58100: Train Loss 0.02622511237859726 | Best Val Loss 0.021282896399497986\n",
      "Step 58200: Train Loss 0.06459861993789673 | Best Val Loss 0.021282896399497986\n",
      "Step 58300: Train Loss 0.034339677542448044 | Best Val Loss 0.021282896399497986\n",
      "Step 58400: Train Loss 0.055805034935474396 | Best Val Loss 0.021282896399497986\n",
      "Step 58500: Train Loss 0.03256406635046005 | Best Val Loss 0.021282896399497986\n",
      "Step 58600: Train Loss 0.024301676079630852 | Best Val Loss 0.021282896399497986\n",
      "Step 58700: Train Loss 0.01784393936395645 | Best Val Loss 0.021282896399497986\n",
      "Step 58800: Train Loss 0.014335747808218002 | Best Val Loss 0.021282896399497986\n",
      "Step 58900: Train Loss 1.128337025642395 | Best Val Loss 0.021282896399497986\n",
      "Step 59000: Validation Loss 0.11844503879547119\n",
      "Step 59000: Train Loss 0.08875001221895218 | Best Val Loss 0.021282896399497986\n",
      "Step 59100: Train Loss 2.1320316791534424 | Best Val Loss 0.021282896399497986\n",
      "Step 59200: Train Loss 0.09021402895450592 | Best Val Loss 0.021282896399497986\n",
      "Step 59300: Train Loss 0.0582188218832016 | Best Val Loss 0.021282896399497986\n",
      "Step 59400: Train Loss 0.1450171172618866 | Best Val Loss 0.021282896399497986\n",
      "Step 59500: Train Loss 0.04919852316379547 | Best Val Loss 0.021282896399497986\n",
      "Step 59600: Train Loss 0.030195629224181175 | Best Val Loss 0.021282896399497986\n",
      "Step 59700: Train Loss 0.033547885715961456 | Best Val Loss 0.021282896399497986\n",
      "Step 59800: Train Loss 0.23133277893066406 | Best Val Loss 0.021282896399497986\n",
      "Step 59900: Train Loss 0.030160626396536827 | Best Val Loss 0.021282896399497986\n",
      "Step 60000: Validation Loss 0.10275810956954956\n",
      "Step 60000: Train Loss 0.019693560898303986 | Best Val Loss 0.021282896399497986\n",
      "Step 60100: Train Loss 0.019581342115998268 | Best Val Loss 0.021282896399497986\n",
      "Step 60200: Train Loss 0.028722142800688744 | Best Val Loss 0.021282896399497986\n",
      "Step 60300: Train Loss 0.02384808100759983 | Best Val Loss 0.021282896399497986\n",
      "Step 60400: Train Loss 0.017587989568710327 | Best Val Loss 0.021282896399497986\n",
      "Step 60500: Train Loss 0.09863466024398804 | Best Val Loss 0.021282896399497986\n",
      "Step 60600: Train Loss 0.34348833560943604 | Best Val Loss 0.021282896399497986\n",
      "Step 60700: Train Loss 0.08508569002151489 | Best Val Loss 0.021282896399497986\n",
      "Step 60800: Train Loss 0.09307372570037842 | Best Val Loss 0.021282896399497986\n",
      "Step 60900: Train Loss 43.43339538574219 | Best Val Loss 0.021282896399497986\n",
      "Step 61000: Validation Loss 0.840726375579834\n",
      "Step 61000: Train Loss 0.6367092132568359 | Best Val Loss 0.021282896399497986\n",
      "Step 61100: Train Loss 0.28797733783721924 | Best Val Loss 0.021282896399497986\n",
      "Step 61200: Train Loss 0.22091445326805115 | Best Val Loss 0.021282896399497986\n",
      "Step 61300: Train Loss 0.14881500601768494 | Best Val Loss 0.021282896399497986\n",
      "Step 61400: Train Loss 0.134214848279953 | Best Val Loss 0.021282896399497986\n",
      "Step 61500: Train Loss 0.1124996542930603 | Best Val Loss 0.021282896399497986\n",
      "Step 61600: Train Loss 0.11739502102136612 | Best Val Loss 0.021282896399497986\n",
      "Step 61700: Train Loss 0.11067333817481995 | Best Val Loss 0.021282896399497986\n",
      "Step 61800: Train Loss 66.61061096191406 | Best Val Loss 0.021282896399497986\n",
      "Step 61900: Train Loss 2.130439043045044 | Best Val Loss 0.021282896399497986\n",
      "Step 62000: Validation Loss 1.7855110168457031\n",
      "Step 62000: Train Loss 1.202963948249817 | Best Val Loss 0.021282896399497986\n",
      "Step 62100: Train Loss 0.5045108795166016 | Best Val Loss 0.021282896399497986\n",
      "Step 62200: Train Loss 0.3081709146499634 | Best Val Loss 0.021282896399497986\n",
      "Step 62300: Train Loss 0.6551607847213745 | Best Val Loss 0.021282896399497986\n",
      "Step 62400: Train Loss 0.11667501926422119 | Best Val Loss 0.021282896399497986\n",
      "Step 62500: Train Loss 0.09721539914608002 | Best Val Loss 0.021282896399497986\n",
      "Step 62600: Train Loss 0.05680326372385025 | Best Val Loss 0.021282896399497986\n",
      "Step 62700: Train Loss 0.6484181880950928 | Best Val Loss 0.021282896399497986\n",
      "Step 62800: Train Loss 0.2570185363292694 | Best Val Loss 0.021282896399497986\n",
      "Step 62900: Train Loss 0.137486532330513 | Best Val Loss 0.021282896399497986\n",
      "Step 63000: Validation Loss 0.13930641114711761\n",
      "Step 63000: Train Loss 0.11370530724525452 | Best Val Loss 0.021282896399497986\n",
      "Step 63100: Train Loss 0.08718419075012207 | Best Val Loss 0.021282896399497986\n",
      "Step 63200: Train Loss 0.055349983274936676 | Best Val Loss 0.021282896399497986\n",
      "Step 63300: Train Loss 0.05446504056453705 | Best Val Loss 0.021282896399497986\n",
      "Step 63400: Train Loss 2.480853319168091 | Best Val Loss 0.021282896399497986\n",
      "Step 63500: Train Loss 0.37803977727890015 | Best Val Loss 0.021282896399497986\n",
      "Step 63600: Train Loss 0.12169119715690613 | Best Val Loss 0.021282896399497986\n",
      "Step 63700: Train Loss 0.08298005163669586 | Best Val Loss 0.021282896399497986\n",
      "Step 63800: Train Loss 0.0631045252084732 | Best Val Loss 0.021282896399497986\n",
      "Step 63900: Train Loss 8.56876277923584 | Best Val Loss 0.021282896399497986\n",
      "Step 64000: Validation Loss 7.2456440925598145\n",
      "Step 64000: Train Loss 5.855978965759277 | Best Val Loss 0.021282896399497986\n",
      "Step 64100: Train Loss 0.2675705552101135 | Best Val Loss 0.021282896399497986\n",
      "Step 64200: Train Loss 0.1654091477394104 | Best Val Loss 0.021282896399497986\n",
      "Step 64300: Train Loss 0.14735591411590576 | Best Val Loss 0.021282896399497986\n",
      "Step 64400: Train Loss 1.477763295173645 | Best Val Loss 0.021282896399497986\n",
      "Step 64500: Train Loss 5.436549186706543 | Best Val Loss 0.021282896399497986\n",
      "Step 64600: Train Loss 1.9427831172943115 | Best Val Loss 0.021282896399497986\n",
      "Step 64700: Train Loss 0.44966283440589905 | Best Val Loss 0.021282896399497986\n",
      "Step 64800: Train Loss 0.27068740129470825 | Best Val Loss 0.021282896399497986\n",
      "Step 64900: Train Loss 0.2290828824043274 | Best Val Loss 0.021282896399497986\n",
      "Step 65000: Validation Loss 0.117414191365242\n",
      "Step 65000: Train Loss 0.08980339765548706 | Best Val Loss 0.021282896399497986\n",
      "Step 65100: Train Loss 0.06866280734539032 | Best Val Loss 0.021282896399497986\n",
      "Step 65200: Train Loss 0.0692531019449234 | Best Val Loss 0.021282896399497986\n",
      "Step 65300: Train Loss 0.08920088410377502 | Best Val Loss 0.021282896399497986\n",
      "Step 65400: Train Loss 0.04068433865904808 | Best Val Loss 0.021282896399497986\n",
      "Step 65500: Train Loss 0.039769671857357025 | Best Val Loss 0.021282896399497986\n",
      "Step 65600: Train Loss 0.03600429370999336 | Best Val Loss 0.021282896399497986\n",
      "Step 65700: Train Loss 0.08969055861234665 | Best Val Loss 0.021282896399497986\n",
      "Step 65800: Train Loss 13.086808204650879 | Best Val Loss 0.021282896399497986\n",
      "Step 65900: Train Loss 0.025591649115085602 | Best Val Loss 0.021282896399497986\n",
      "Step 66000: Validation Loss 0.0465845949947834\n",
      "Step 66000: Train Loss 0.02441016212105751 | Best Val Loss 0.021282896399497986\n",
      "Step 66100: Train Loss 0.8610244393348694 | Best Val Loss 0.021282896399497986\n",
      "Step 66200: Train Loss 0.37721049785614014 | Best Val Loss 0.021282896399497986\n",
      "Step 66300: Train Loss 8.955692291259766 | Best Val Loss 0.021282896399497986\n",
      "Step 66400: Train Loss 0.5767207145690918 | Best Val Loss 0.021282896399497986\n",
      "Step 66500: Train Loss 22.15391731262207 | Best Val Loss 0.021282896399497986\n",
      "Step 66600: Train Loss 1.710439920425415 | Best Val Loss 0.021282896399497986\n",
      "Step 66700: Train Loss 0.34805795550346375 | Best Val Loss 0.021282896399497986\n",
      "Step 66800: Train Loss 0.21468505263328552 | Best Val Loss 0.021282896399497986\n",
      "Step 66900: Train Loss 0.09643332660198212 | Best Val Loss 0.021282896399497986\n",
      "Step 67000: Validation Loss 0.6049179434776306\n",
      "Step 67000: Train Loss 0.5936481952667236 | Best Val Loss 0.021282896399497986\n",
      "Step 67100: Train Loss 0.08755022287368774 | Best Val Loss 0.021282896399497986\n",
      "Step 67200: Train Loss 0.10052657127380371 | Best Val Loss 0.021282896399497986\n",
      "Step 67300: Train Loss 0.06275862455368042 | Best Val Loss 0.021282896399497986\n",
      "Step 67400: Train Loss 1.6118810176849365 | Best Val Loss 0.021282896399497986\n",
      "Step 67500: Train Loss 0.3210935592651367 | Best Val Loss 0.021282896399497986\n",
      "Step 67600: Train Loss 0.22077816724777222 | Best Val Loss 0.021282896399497986\n",
      "Step 67700: Train Loss 0.09905664622783661 | Best Val Loss 0.021282896399497986\n",
      "Step 67800: Train Loss 0.1024029403924942 | Best Val Loss 0.021282896399497986\n",
      "Step 67900: Train Loss 0.10579098761081696 | Best Val Loss 0.021282896399497986\n",
      "Step 68000: Validation Loss 0.2832864820957184\n",
      "Step 68000: Train Loss 0.16456004977226257 | Best Val Loss 0.021282896399497986\n",
      "Step 68100: Train Loss 0.13925936818122864 | Best Val Loss 0.021282896399497986\n",
      "Step 68200: Train Loss 0.12316417694091797 | Best Val Loss 0.021282896399497986\n",
      "Step 68300: Train Loss 0.08169161528348923 | Best Val Loss 0.021282896399497986\n",
      "Step 68400: Train Loss 0.04161309823393822 | Best Val Loss 0.021282896399497986\n",
      "Step 68500: Train Loss 0.0694701224565506 | Best Val Loss 0.021282896399497986\n",
      "Step 68600: Train Loss 0.5367439985275269 | Best Val Loss 0.021282896399497986\n",
      "Step 68700: Train Loss 0.2658921182155609 | Best Val Loss 0.021282896399497986\n",
      "Step 68800: Train Loss 0.0739213079214096 | Best Val Loss 0.021282896399497986\n",
      "Step 68900: Train Loss 45.5939826965332 | Best Val Loss 0.021282896399497986\n",
      "Step 69000: Validation Loss 0.057583242654800415\n",
      "Step 69000: Train Loss 0.04073864966630936 | Best Val Loss 0.021282896399497986\n",
      "Step 69100: Train Loss 0.0245559923350811 | Best Val Loss 0.021282896399497986\n",
      "Step 69200: Train Loss 0.0285232812166214 | Best Val Loss 0.021282896399497986\n",
      "Step 69300: Train Loss 0.13492651283740997 | Best Val Loss 0.021282896399497986\n",
      "Step 69400: Train Loss 0.10704585909843445 | Best Val Loss 0.021282896399497986\n",
      "Step 69500: Train Loss 0.03606175631284714 | Best Val Loss 0.021282896399497986\n",
      "Step 69600: Train Loss 0.01958327367901802 | Best Val Loss 0.021282896399497986\n",
      "Step 69700: Train Loss 0.6206272840499878 | Best Val Loss 0.021282896399497986\n",
      "Step 69800: Train Loss 0.01664368435740471 | Best Val Loss 0.021282896399497986\n",
      "Step 69900: Train Loss 0.014253457076847553 | Best Val Loss 0.021282896399497986\n",
      "Step 70000: Validation Loss 0.02813626267015934\n",
      "Step 70000: Train Loss 0.013825852423906326 | Best Val Loss 0.021282896399497986\n",
      "Step 70100: Train Loss 0.011305252090096474 | Best Val Loss 0.021282896399497986\n",
      "Step 70200: Train Loss 0.010946793481707573 | Best Val Loss 0.021282896399497986\n",
      "Step 70300: Train Loss 0.0748690813779831 | Best Val Loss 0.021282896399497986\n",
      "Step 70400: Train Loss 0.04168673977255821 | Best Val Loss 0.021282896399497986\n",
      "Step 70500: Train Loss 0.02788742631673813 | Best Val Loss 0.021282896399497986\n",
      "Step 70600: Train Loss 0.029106220230460167 | Best Val Loss 0.021282896399497986\n",
      "Step 70700: Train Loss 0.03896104544401169 | Best Val Loss 0.021282896399497986\n",
      "Step 70800: Train Loss 0.018388068303465843 | Best Val Loss 0.021282896399497986\n",
      "Step 70900: Train Loss 0.02909266948699951 | Best Val Loss 0.021282896399497986\n",
      "Step 71000: Validation Loss 0.03272577002644539\n",
      "Step 71000: Train Loss 0.018308671191334724 | Best Val Loss 0.021282896399497986\n",
      "Step 71100: Train Loss 0.016424600034952164 | Best Val Loss 0.021282896399497986\n",
      "Step 71200: Train Loss 0.016030721366405487 | Best Val Loss 0.021282896399497986\n",
      "Step 71300: Train Loss 0.020519066601991653 | Best Val Loss 0.021282896399497986\n",
      "Step 71400: Train Loss 0.01867573708295822 | Best Val Loss 0.021282896399497986\n",
      "Step 71500: Train Loss 0.016816046088933945 | Best Val Loss 0.021282896399497986\n",
      "Step 71600: Train Loss 0.02012789621949196 | Best Val Loss 0.021282896399497986\n",
      "Step 71700: Train Loss 0.012506161816418171 | Best Val Loss 0.021282896399497986\n",
      "Step 71800: Train Loss 0.10637867450714111 | Best Val Loss 0.021282896399497986\n",
      "Step 71900: Train Loss 0.07893849164247513 | Best Val Loss 0.021282896399497986\n",
      "Step 72000: Validation Loss 0.04659528285264969\n",
      "Step 72000: Train Loss 0.03037836402654648 | Best Val Loss 0.021282896399497986\n",
      "Step 72100: Train Loss 0.025171615183353424 | Best Val Loss 0.021282896399497986\n",
      "Step 72200: Train Loss 0.0234806127846241 | Best Val Loss 0.021282896399497986\n",
      "Step 72300: Train Loss 0.016590993851423264 | Best Val Loss 0.021282896399497986\n",
      "Step 72400: Train Loss 0.018584107980132103 | Best Val Loss 0.021282896399497986\n",
      "Step 72500: Train Loss 0.02657921239733696 | Best Val Loss 0.021282896399497986\n",
      "Step 72600: Train Loss 0.00906828511506319 | Best Val Loss 0.021282896399497986\n",
      "Step 72700: Train Loss 0.15750598907470703 | Best Val Loss 0.021282896399497986\n",
      "Step 72800: Train Loss 0.014736737124621868 | Best Val Loss 0.021282896399497986\n",
      "Step 72900: Train Loss 0.00942818820476532 | Best Val Loss 0.021282896399497986\n",
      "Step 73000: Validation Loss 0.1842435896396637\n",
      "Step 73000: Train Loss 0.1257701963186264 | Best Val Loss 0.021282896399497986\n",
      "Step 73100: Train Loss 0.14524058997631073 | Best Val Loss 0.021282896399497986\n",
      "Step 73200: Train Loss 0.014722646214067936 | Best Val Loss 0.021282896399497986\n",
      "Step 73300: Train Loss 0.010677742771804333 | Best Val Loss 0.021282896399497986\n",
      "Step 73400: Train Loss 0.030964773148298264 | Best Val Loss 0.021282896399497986\n",
      "Step 73500: Train Loss 0.016666360199451447 | Best Val Loss 0.021282896399497986\n",
      "Step 73600: Train Loss 0.013065424747765064 | Best Val Loss 0.021282896399497986\n",
      "Step 73700: Train Loss 0.030511867254972458 | Best Val Loss 0.021282896399497986\n",
      "Step 73800: Train Loss 0.8883723616600037 | Best Val Loss 0.021282896399497986\n",
      "Step 73900: Train Loss 0.02082875370979309 | Best Val Loss 0.021282896399497986\n",
      "Step 74000: Validation Loss 0.02767159789800644\n",
      "Step 74000: Train Loss 0.014879782684147358 | Best Val Loss 0.021282896399497986\n",
      "Step 74100: Train Loss 0.010415407828986645 | Best Val Loss 0.021282896399497986\n",
      "Step 74200: Train Loss 4.438765048980713 | Best Val Loss 0.021282896399497986\n",
      "Step 74300: Train Loss 0.1343306303024292 | Best Val Loss 0.021282896399497986\n",
      "Step 74400: Train Loss 0.028706613928079605 | Best Val Loss 0.021282896399497986\n",
      "Step 74500: Train Loss 0.09217886626720428 | Best Val Loss 0.021282896399497986\n",
      "Step 74600: Train Loss 0.02912272699177265 | Best Val Loss 0.021282896399497986\n",
      "Step 74700: Train Loss 0.015538162551820278 | Best Val Loss 0.021282896399497986\n",
      "Step 74800: Train Loss 0.012681503780186176 | Best Val Loss 0.021282896399497986\n",
      "Step 74900: Train Loss 0.009003689512610435 | Best Val Loss 0.021282896399497986\n",
      "Step 75000: Validation Loss 0.07082517445087433\n",
      "Step 75000: Train Loss 0.04362667351961136 | Best Val Loss 0.021282896399497986\n",
      "Step 75100: Train Loss 0.17356398701667786 | Best Val Loss 0.021282896399497986\n",
      "Step 75200: Train Loss 0.12351009994745255 | Best Val Loss 0.021282896399497986\n",
      "Step 75300: Train Loss 0.1115640327334404 | Best Val Loss 0.021282896399497986\n",
      "Step 75400: Train Loss 0.05330715700984001 | Best Val Loss 0.021282896399497986\n",
      "Step 75500: Train Loss 0.011421632021665573 | Best Val Loss 0.021282896399497986\n",
      "Step 75600: Train Loss 0.00999366957694292 | Best Val Loss 0.021282896399497986\n",
      "Step 75700: Train Loss 0.01264452375471592 | Best Val Loss 0.021282896399497986\n",
      "Step 75800: Train Loss 0.03871215507388115 | Best Val Loss 0.021282896399497986\n",
      "Step 75900: Train Loss 0.023533497005701065 | Best Val Loss 0.021282896399497986\n",
      "Step 76000: Validation Loss 0.03800405561923981\n",
      "Step 76000: Train Loss 0.02489018440246582 | Best Val Loss 0.021282896399497986\n",
      "Step 76100: Train Loss 0.6679816246032715 | Best Val Loss 0.021282896399497986\n",
      "Step 76200: Train Loss 0.07650576531887054 | Best Val Loss 0.021282896399497986\n",
      "Step 76300: Train Loss 0.26200270652770996 | Best Val Loss 0.021282896399497986\n",
      "Step 76400: Train Loss 0.02364632487297058 | Best Val Loss 0.021282896399497986\n",
      "Step 76500: Train Loss 0.01418250985443592 | Best Val Loss 0.021282896399497986\n",
      "Step 76600: Train Loss 0.013921951875090599 | Best Val Loss 0.021282896399497986\n",
      "Step 76700: Train Loss 0.010103605687618256 | Best Val Loss 0.021282896399497986\n",
      "Step 76800: Train Loss 0.16046351194381714 | Best Val Loss 0.021282896399497986\n",
      "Step 76900: Train Loss 0.010241789743304253 | Best Val Loss 0.021282896399497986\n",
      "Step 77000: Validation Loss 0.028663666918873787\n",
      "Step 77000: Train Loss 0.007756790146231651 | Best Val Loss 0.021282896399497986\n",
      "Step 77100: Train Loss 0.007684101350605488 | Best Val Loss 0.021282896399497986\n",
      "Step 77200: Train Loss 0.007829196751117706 | Best Val Loss 0.021282896399497986\n",
      "Step 77300: Train Loss 0.0048121120780706406 | Best Val Loss 0.021282896399497986\n",
      "Step 77400: Train Loss 0.005652321502566338 | Best Val Loss 0.021282896399497986\n",
      "Step 77500: Train Loss 0.1320592612028122 | Best Val Loss 0.021282896399497986\n",
      "Step 77600: Train Loss 0.015146676450967789 | Best Val Loss 0.021282896399497986\n",
      "Step 77700: Train Loss 0.011680180206894875 | Best Val Loss 0.021282896399497986\n",
      "Step 77800: Train Loss 0.011021208018064499 | Best Val Loss 0.021282896399497986\n",
      "Step 77900: Train Loss 0.06385871767997742 | Best Val Loss 0.021282896399497986\n",
      "Step 78000: Validation Loss 0.052317604422569275\n",
      "Step 78000: Train Loss 0.03400089964270592 | Best Val Loss 0.021282896399497986\n",
      "Step 78100: Train Loss 0.05187495797872543 | Best Val Loss 0.021282896399497986\n",
      "Step 78200: Train Loss 0.021713528782129288 | Best Val Loss 0.021282896399497986\n",
      "Step 78300: Train Loss 0.012394166551530361 | Best Val Loss 0.021282896399497986\n",
      "Step 78400: Train Loss 0.09789206087589264 | Best Val Loss 0.021282896399497986\n",
      "Step 78500: Train Loss 0.02250429056584835 | Best Val Loss 0.021282896399497986\n",
      "Step 78600: Train Loss 0.012722186744213104 | Best Val Loss 0.021282896399497986\n",
      "Step 78700: Train Loss 0.010041512548923492 | Best Val Loss 0.021282896399497986\n",
      "Step 78800: Train Loss 0.00742096733301878 | Best Val Loss 0.021282896399497986\n",
      "Step 78900: Train Loss 0.05856657028198242 | Best Val Loss 0.021282896399497986\n",
      "Step 79000: Validation Loss 0.03412119299173355\n",
      "Step 79000: Train Loss 0.02013784646987915 | Best Val Loss 0.021282896399497986\n",
      "Step 79100: Train Loss 0.04948992282152176 | Best Val Loss 0.021282896399497986\n",
      "Step 79200: Train Loss 0.01990336924791336 | Best Val Loss 0.021282896399497986\n",
      "Step 79300: Train Loss 0.013800621964037418 | Best Val Loss 0.021282896399497986\n",
      "Step 79400: Train Loss 0.009676462039351463 | Best Val Loss 0.021282896399497986\n",
      "Step 79500: Train Loss 0.012229000218212605 | Best Val Loss 0.021282896399497986\n",
      "Step 79600: Train Loss 0.0286225788295269 | Best Val Loss 0.021282896399497986\n",
      "Step 79700: Train Loss 0.04130521044135094 | Best Val Loss 0.021282896399497986\n",
      "Step 79800: Train Loss 0.019864056259393692 | Best Val Loss 0.021282896399497986\n",
      "Step 79900: Train Loss 0.012035490944981575 | Best Val Loss 0.021282896399497986\n",
      "Step 80000: Validation Loss 0.14563967287540436\n",
      "Step 80000: Train Loss 0.09285099804401398 | Best Val Loss 0.021282896399497986\n",
      "Step 80100: Train Loss 0.03191713988780975 | Best Val Loss 0.021282896399497986\n",
      "Step 80200: Train Loss 0.01768368110060692 | Best Val Loss 0.021282896399497986\n",
      "Step 80300: Train Loss 0.01429089717566967 | Best Val Loss 0.021282896399497986\n",
      "Step 80400: Train Loss 0.006807650439441204 | Best Val Loss 0.021282896399497986\n",
      "Step 80500: Train Loss 0.04774167388677597 | Best Val Loss 0.021282896399497986\n",
      "Step 80600: Train Loss 0.013458753004670143 | Best Val Loss 0.021282896399497986\n",
      "Step 80700: Train Loss 0.013177945278584957 | Best Val Loss 0.021282896399497986\n",
      "Step 80800: Train Loss 0.007062089629471302 | Best Val Loss 0.021282896399497986\n",
      "Step 80900: Train Loss 0.00634155934676528 | Best Val Loss 0.021282896399497986\n",
      "Step 81000: Validation Loss 0.024944813922047615\n",
      "Step 81000: Train Loss 0.005697199609130621 | Best Val Loss 0.021282896399497986\n",
      "Step 81100: Train Loss 0.004614600446075201 | Best Val Loss 0.021282896399497986\n",
      "Step 81200: Train Loss 0.0063081467524170876 | Best Val Loss 0.021282896399497986\n",
      "Step 81300: Train Loss 0.023448005318641663 | Best Val Loss 0.021282896399497986\n",
      "Step 81400: Train Loss 0.01117384247481823 | Best Val Loss 0.021282896399497986\n",
      "Step 81500: Train Loss 0.02133708819746971 | Best Val Loss 0.021282896399497986\n",
      "Step 81600: Train Loss 0.05441432446241379 | Best Val Loss 0.021282896399497986\n",
      "Step 81700: Train Loss 0.024010471999645233 | Best Val Loss 0.021282896399497986\n",
      "Step 81800: Train Loss 0.017993487417697906 | Best Val Loss 0.021282896399497986\n",
      "Step 81900: Train Loss 0.009497453458607197 | Best Val Loss 0.021282896399497986\n",
      "Step 82000: Validation Loss 0.03324410691857338\n",
      "Step 82000: Train Loss 0.0250076986849308 | Best Val Loss 0.021282896399497986\n",
      "Step 82100: Train Loss 0.020164581015706062 | Best Val Loss 0.021282896399497986\n",
      "Step 82200: Train Loss 0.01787896826863289 | Best Val Loss 0.021282896399497986\n",
      "Step 82300: Train Loss 0.0320163369178772 | Best Val Loss 0.021282896399497986\n",
      "Step 82400: Train Loss 0.012110467068850994 | Best Val Loss 0.021282896399497986\n",
      "Step 82500: Train Loss 0.015054850839078426 | Best Val Loss 0.021282896399497986\n",
      "Step 82600: Train Loss 0.06920195370912552 | Best Val Loss 0.021282896399497986\n",
      "Step 82700: Train Loss 0.03497470170259476 | Best Val Loss 0.021282896399497986\n",
      "Step 82800: Train Loss 0.027535799890756607 | Best Val Loss 0.021282896399497986\n",
      "Step 82900: Train Loss 0.020587943494319916 | Best Val Loss 0.021282896399497986\n",
      "Step 83000: Validation Loss 0.025672242045402527\n",
      "Step 83000: Train Loss 0.012609859928488731 | Best Val Loss 0.021282896399497986\n",
      "Step 83100: Train Loss 0.2575814127922058 | Best Val Loss 0.021282896399497986\n",
      "Step 83200: Train Loss 0.33021149039268494 | Best Val Loss 0.021282896399497986\n",
      "Step 83300: Train Loss 0.0903186947107315 | Best Val Loss 0.021282896399497986\n",
      "Step 83400: Train Loss 0.06533988565206528 | Best Val Loss 0.021282896399497986\n",
      "Step 83500: Train Loss 0.015233480371534824 | Best Val Loss 0.021282896399497986\n",
      "Step 83600: Train Loss 0.010162224993109703 | Best Val Loss 0.021282896399497986\n",
      "Step 83700: Train Loss 0.00939544290304184 | Best Val Loss 0.021282896399497986\n",
      "Step 83800: Train Loss 0.011240864172577858 | Best Val Loss 0.021282896399497986\n",
      "Step 83900: Train Loss 0.15264376997947693 | Best Val Loss 0.021282896399497986\n",
      "Step 84000: Validation Loss 0.053187884390354156\n",
      "Step 84000: Train Loss 0.04320400953292847 | Best Val Loss 0.021282896399497986\n",
      "Step 84100: Train Loss 0.44709083437919617 | Best Val Loss 0.021282896399497986\n",
      "Step 84200: Train Loss 0.024172883480787277 | Best Val Loss 0.021282896399497986\n",
      "Step 84300: Train Loss 106.7647476196289 | Best Val Loss 0.021282896399497986\n",
      "Step 84400: Train Loss 0.8900741338729858 | Best Val Loss 0.021282896399497986\n",
      "Step 84500: Train Loss 8.317867279052734 | Best Val Loss 0.021282896399497986\n",
      "Step 84600: Train Loss 5.264681816101074 | Best Val Loss 0.021282896399497986\n",
      "Step 84700: Train Loss 0.9853729605674744 | Best Val Loss 0.021282896399497986\n",
      "Step 84800: Train Loss 0.32662564516067505 | Best Val Loss 0.021282896399497986\n",
      "Step 84900: Train Loss 5.793392181396484 | Best Val Loss 0.021282896399497986\n",
      "Step 85000: Validation Loss 1.2305647134780884\n",
      "Step 85000: Train Loss 1.087578296661377 | Best Val Loss 0.021282896399497986\n",
      "Step 85100: Train Loss 0.8451639413833618 | Best Val Loss 0.021282896399497986\n",
      "Step 85200: Train Loss 1.619847059249878 | Best Val Loss 0.021282896399497986\n",
      "Step 85300: Train Loss 10.395193099975586 | Best Val Loss 0.021282896399497986\n",
      "Step 85400: Train Loss 1.4065046310424805 | Best Val Loss 0.021282896399497986\n",
      "Step 85500: Train Loss 0.5175747871398926 | Best Val Loss 0.021282896399497986\n",
      "Step 85600: Train Loss 1.3835537433624268 | Best Val Loss 0.021282896399497986\n",
      "Step 85700: Train Loss 0.48027175664901733 | Best Val Loss 0.021282896399497986\n",
      "Step 85800: Train Loss 0.37189173698425293 | Best Val Loss 0.021282896399497986\n",
      "Step 85900: Train Loss 0.2934061884880066 | Best Val Loss 0.021282896399497986\n",
      "Step 86000: Validation Loss 0.1919289529323578\n",
      "Step 86000: Train Loss 0.09915000200271606 | Best Val Loss 0.021282896399497986\n",
      "Step 86100: Train Loss 0.09455208480358124 | Best Val Loss 0.021282896399497986\n",
      "Step 86200: Train Loss 1.1516633033752441 | Best Val Loss 0.021282896399497986\n",
      "Step 86300: Train Loss 0.3989148736000061 | Best Val Loss 0.021282896399497986\n",
      "Step 86400: Train Loss 0.2549893856048584 | Best Val Loss 0.021282896399497986\n",
      "Step 86500: Train Loss 0.17456303536891937 | Best Val Loss 0.021282896399497986\n",
      "Step 86600: Train Loss 0.2825292646884918 | Best Val Loss 0.021282896399497986\n",
      "Step 86700: Train Loss 0.08844694495201111 | Best Val Loss 0.021282896399497986\n",
      "Step 86800: Train Loss 0.04922805726528168 | Best Val Loss 0.021282896399497986\n",
      "Step 86900: Train Loss 0.07437051832675934 | Best Val Loss 0.021282896399497986\n",
      "Step 87000: Validation Loss 1.4531692266464233\n",
      "Step 87000: Train Loss 1.253222942352295 | Best Val Loss 0.021282896399497986\n",
      "Step 87100: Train Loss 1.7868685722351074 | Best Val Loss 0.021282896399497986\n",
      "Step 87200: Train Loss 0.11095428466796875 | Best Val Loss 0.021282896399497986\n",
      "Step 87300: Train Loss 0.09003038704395294 | Best Val Loss 0.021282896399497986\n",
      "Step 87400: Train Loss 0.3320220112800598 | Best Val Loss 0.021282896399497986\n",
      "Step 87500: Train Loss 0.2626034617424011 | Best Val Loss 0.021282896399497986\n",
      "Step 87600: Train Loss 0.05351682007312775 | Best Val Loss 0.021282896399497986\n",
      "Step 87700: Train Loss 0.04672398418188095 | Best Val Loss 0.021282896399497986\n",
      "Step 87800: Train Loss 0.04198054224252701 | Best Val Loss 0.021282896399497986\n",
      "Step 87900: Train Loss 0.06446990370750427 | Best Val Loss 0.021282896399497986\n",
      "Step 88000: Validation Loss 0.06193213909864426\n",
      "Step 88000: Train Loss 0.044949986040592194 | Best Val Loss 0.021282896399497986\n",
      "Step 88100: Train Loss 0.0643642246723175 | Best Val Loss 0.021282896399497986\n",
      "Step 88200: Train Loss 0.04227998107671738 | Best Val Loss 0.021282896399497986\n",
      "Step 88300: Train Loss 0.04282160475850105 | Best Val Loss 0.021282896399497986\n",
      "Step 88400: Train Loss 0.0380583330988884 | Best Val Loss 0.021282896399497986\n",
      "Step 88500: Train Loss 0.031418222934007645 | Best Val Loss 0.021282896399497986\n",
      "Step 88600: Train Loss 0.03051910549402237 | Best Val Loss 0.021282896399497986\n",
      "Step 88700: Train Loss 0.0246804878115654 | Best Val Loss 0.021282896399497986\n",
      "Step 88800: Train Loss 0.032315853983163834 | Best Val Loss 0.021282896399497986\n",
      "Step 88900: Train Loss 0.03438838571310043 | Best Val Loss 0.021282896399497986\n",
      "Step 89000: Validation Loss 0.13564534485340118\n",
      "Step 89000: Train Loss 0.10033959895372391 | Best Val Loss 0.021282896399497986\n",
      "Step 89100: Train Loss 0.03396153822541237 | Best Val Loss 0.021282896399497986\n",
      "Step 89200: Train Loss 0.03832544386386871 | Best Val Loss 0.021282896399497986\n",
      "Step 89300: Train Loss 0.03922966122627258 | Best Val Loss 0.021282896399497986\n",
      "Step 89400: Train Loss 0.031231556087732315 | Best Val Loss 0.021282896399497986\n",
      "Step 89500: Train Loss 0.028972825035452843 | Best Val Loss 0.021282896399497986\n",
      "Step 89600: Train Loss 0.06606438755989075 | Best Val Loss 0.021282896399497986\n",
      "Step 89700: Train Loss 0.0467204675078392 | Best Val Loss 0.021282896399497986\n",
      "Step 89800: Train Loss 0.03916645422577858 | Best Val Loss 0.021282896399497986\n",
      "Step 89900: Train Loss 0.10174541920423508 | Best Val Loss 0.021282896399497986\n",
      "Step 90000: Validation Loss 0.039465710520744324\n",
      "Step 90000: Train Loss 0.026156429201364517 | Best Val Loss 0.021282896399497986\n",
      "Step 90100: Train Loss 0.01937917061150074 | Best Val Loss 0.021282896399497986\n",
      "Step 90200: Train Loss 51.14830780029297 | Best Val Loss 0.021282896399497986\n",
      "Step 90300: Train Loss 0.03282774239778519 | Best Val Loss 0.021282896399497986\n",
      "Step 90400: Train Loss 0.029711734503507614 | Best Val Loss 0.021282896399497986\n",
      "Step 90500: Train Loss 0.03333180025219917 | Best Val Loss 0.021282896399497986\n",
      "Step 90600: Train Loss 0.03157275915145874 | Best Val Loss 0.021282896399497986\n",
      "Step 90700: Train Loss 0.023917172104120255 | Best Val Loss 0.021282896399497986\n",
      "Step 90800: Train Loss 0.024459954351186752 | Best Val Loss 0.021282896399497986\n",
      "Step 90900: Train Loss 0.022984866052865982 | Best Val Loss 0.021282896399497986\n",
      "Step 91000: Validation Loss 0.03757091239094734\n",
      "Step 91000: Train Loss 0.02225188910961151 | Best Val Loss 0.021282896399497986\n",
      "Step 91100: Train Loss 0.024021459743380547 | Best Val Loss 0.021282896399497986\n",
      "Step 91200: Train Loss 0.025683369487524033 | Best Val Loss 0.021282896399497986\n",
      "Step 91300: Train Loss 0.018386729061603546 | Best Val Loss 0.021282896399497986\n",
      "Step 91400: Train Loss 0.021421147510409355 | Best Val Loss 0.021282896399497986\n",
      "Step 91500: Train Loss 0.08005467057228088 | Best Val Loss 0.021282896399497986\n",
      "Step 91600: Train Loss 0.03495703265070915 | Best Val Loss 0.021282896399497986\n",
      "Step 91700: Train Loss 0.0235236007720232 | Best Val Loss 0.021282896399497986\n",
      "Step 91800: Train Loss 0.02256888523697853 | Best Val Loss 0.021282896399497986\n",
      "Step 91900: Train Loss 0.025966329500079155 | Best Val Loss 0.021282896399497986\n",
      "Step 92000: Validation Loss 0.03581872954964638\n",
      "Step 92000: Train Loss 0.02417542226612568 | Best Val Loss 0.021282896399497986\n",
      "Step 92100: Train Loss 0.022452566772699356 | Best Val Loss 0.021282896399497986\n",
      "Step 92200: Train Loss 0.026881922036409378 | Best Val Loss 0.021282896399497986\n",
      "Step 92300: Train Loss 0.01786268688738346 | Best Val Loss 0.021282896399497986\n",
      "Step 92400: Train Loss 0.014857782050967216 | Best Val Loss 0.021282896399497986\n",
      "Step 92500: Train Loss 0.05852722376585007 | Best Val Loss 0.021282896399497986\n",
      "Step 92600: Train Loss 0.034840065985918045 | Best Val Loss 0.021282896399497986\n",
      "Step 92700: Train Loss 0.022288959473371506 | Best Val Loss 0.021282896399497986\n",
      "Step 92800: Train Loss 0.07829432189464569 | Best Val Loss 0.021282896399497986\n",
      "Step 92900: Train Loss 0.018644265830516815 | Best Val Loss 0.021282896399497986\n",
      "Step 93000: Validation Loss 0.1277211755514145\n",
      "Step 93000: Train Loss 0.08839583396911621 | Best Val Loss 0.021282896399497986\n",
      "Step 93100: Train Loss 0.047018781304359436 | Best Val Loss 0.021282896399497986\n",
      "Step 93200: Train Loss 0.018094953149557114 | Best Val Loss 0.021282896399497986\n",
      "Step 93300: Train Loss 0.01957160048186779 | Best Val Loss 0.021282896399497986\n",
      "Step 93400: Train Loss 0.26906856894493103 | Best Val Loss 0.021282896399497986\n",
      "Step 93500: Train Loss 0.060131363570690155 | Best Val Loss 0.021282896399497986\n",
      "Step 93600: Train Loss 0.05010906606912613 | Best Val Loss 0.021282896399497986\n",
      "Step 93700: Train Loss 0.4499555826187134 | Best Val Loss 0.021282896399497986\n",
      "Step 93800: Train Loss 0.08628155291080475 | Best Val Loss 0.021282896399497986\n",
      "Step 93900: Train Loss 0.0564022995531559 | Best Val Loss 0.021282896399497986\n",
      "Step 94000: Validation Loss 0.045851532369852066\n",
      "Step 94000: Train Loss 0.031095240265130997 | Best Val Loss 0.021282896399497986\n",
      "Step 94100: Train Loss 0.02347986213862896 | Best Val Loss 0.021282896399497986\n",
      "Step 94200: Train Loss 0.5209469795227051 | Best Val Loss 0.021282896399497986\n",
      "Step 94300: Train Loss 0.02542654611170292 | Best Val Loss 0.021282896399497986\n",
      "Step 94400: Train Loss 0.015536975115537643 | Best Val Loss 0.021282896399497986\n",
      "Step 94500: Train Loss 0.08005964756011963 | Best Val Loss 0.021282896399497986\n",
      "Step 94600: Train Loss 0.06975455582141876 | Best Val Loss 0.021282896399497986\n",
      "Step 94700: Train Loss 0.039321187883615494 | Best Val Loss 0.021282896399497986\n",
      "Step 94800: Train Loss 0.03330019861459732 | Best Val Loss 0.021282896399497986\n",
      "Step 94900: Train Loss 0.032786257565021515 | Best Val Loss 0.021282896399497986\n",
      "Step 95000: Validation Loss 0.03846017271280289\n",
      "Step 95000: Train Loss 0.02255094051361084 | Best Val Loss 0.021282896399497986\n",
      "Step 95100: Train Loss 0.02576531283557415 | Best Val Loss 0.021282896399497986\n",
      "Step 95200: Train Loss 0.021327583119273186 | Best Val Loss 0.021282896399497986\n",
      "Step 95300: Train Loss 0.11070418357849121 | Best Val Loss 0.021282896399497986\n",
      "Step 95400: Train Loss 0.06253935396671295 | Best Val Loss 0.021282896399497986\n",
      "Step 95500: Train Loss 0.06155736744403839 | Best Val Loss 0.021282896399497986\n",
      "Step 95600: Train Loss 0.0394643172621727 | Best Val Loss 0.021282896399497986\n",
      "Step 95700: Train Loss 0.02922360599040985 | Best Val Loss 0.021282896399497986\n",
      "Step 95800: Train Loss 0.021522557362914085 | Best Val Loss 0.021282896399497986\n",
      "Step 95900: Train Loss 0.018544645980000496 | Best Val Loss 0.021282896399497986\n",
      "Step 96000: Validation Loss 0.03965187817811966\n",
      "Step 96000: Train Loss 0.02424360066652298 | Best Val Loss 0.021282896399497986\n",
      "Step 96100: Train Loss 0.016519833356142044 | Best Val Loss 0.021282896399497986\n",
      "Step 96200: Train Loss 0.012049529701471329 | Best Val Loss 0.021282896399497986\n",
      "Step 96300: Train Loss 0.009793730452656746 | Best Val Loss 0.021282896399497986\n",
      "Step 96400: Train Loss 0.009123343974351883 | Best Val Loss 0.021282896399497986\n",
      "Step 96500: Train Loss 0.0116072166711092 | Best Val Loss 0.021282896399497986\n",
      "Step 96600: Train Loss 0.011217527091503143 | Best Val Loss 0.021282896399497986\n",
      "Step 96700: Train Loss 0.01909303478896618 | Best Val Loss 0.021282896399497986\n",
      "Step 96800: Train Loss 0.016020290553569794 | Best Val Loss 0.021282896399497986\n",
      "Step 96900: Train Loss 0.010915578342974186 | Best Val Loss 0.021282896399497986\n",
      "Step 97000: Validation Loss 0.032919056713581085\n",
      "Step 97000: Train Loss 0.013130789622664452 | Best Val Loss 0.021282896399497986\n",
      "Step 97100: Train Loss 0.01660989224910736 | Best Val Loss 0.021282896399497986\n",
      "Step 97200: Train Loss 0.10593827813863754 | Best Val Loss 0.021282896399497986\n",
      "Step 97300: Train Loss 0.06878840178251266 | Best Val Loss 0.021282896399497986\n",
      "Step 97400: Train Loss 0.0496613010764122 | Best Val Loss 0.021282896399497986\n",
      "Step 97500: Train Loss 0.04273359477519989 | Best Val Loss 0.021282896399497986\n",
      "Step 97600: Train Loss 0.025690428912639618 | Best Val Loss 0.021282896399497986\n",
      "Step 97700: Train Loss 0.01532465685158968 | Best Val Loss 0.021282896399497986\n",
      "Step 97800: Train Loss 0.016428355127573013 | Best Val Loss 0.021282896399497986\n",
      "Step 97900: Train Loss 0.014143131673336029 | Best Val Loss 0.021282896399497986\n",
      "Step 98000: Validation Loss 0.02684098854660988\n",
      "Step 98000: Train Loss 0.009384403005242348 | Best Val Loss 0.021282896399497986\n",
      "Step 98100: Train Loss 0.008671453222632408 | Best Val Loss 0.021282896399497986\n",
      "Step 98200: Train Loss 0.013427231460809708 | Best Val Loss 0.021282896399497986\n",
      "Step 98300: Train Loss 0.0075272382237017155 | Best Val Loss 0.021282896399497986\n",
      "Step 98400: Train Loss 0.011343671940267086 | Best Val Loss 0.021282896399497986\n",
      "Step 98500: Train Loss 1.5254740715026855 | Best Val Loss 0.021282896399497986\n",
      "Step 98600: Train Loss 0.041127774864435196 | Best Val Loss 0.021282896399497986\n",
      "Step 98700: Train Loss 0.9208742380142212 | Best Val Loss 0.021282896399497986\n",
      "Step 98800: Train Loss 0.1071062907576561 | Best Val Loss 0.021282896399497986\n",
      "Step 98900: Train Loss 0.058299772441387177 | Best Val Loss 0.021282896399497986\n",
      "Step 99000: Validation Loss 0.22806210815906525\n",
      "Step 99000: Train Loss 0.15791860222816467 | Best Val Loss 0.021282896399497986\n",
      "Step 99100: Train Loss 0.09232883900403976 | Best Val Loss 0.021282896399497986\n",
      "Step 99200: Train Loss 0.04997074231505394 | Best Val Loss 0.021282896399497986\n",
      "Step 99300: Train Loss 0.0394735261797905 | Best Val Loss 0.021282896399497986\n",
      "Step 99400: Train Loss 0.028042225167155266 | Best Val Loss 0.021282896399497986\n",
      "Step 99500: Train Loss 0.02637779340147972 | Best Val Loss 0.021282896399497986\n",
      "Step 99600: Train Loss 0.022787556052207947 | Best Val Loss 0.021282896399497986\n",
      "Step 99700: Train Loss 0.022759124636650085 | Best Val Loss 0.021282896399497986\n",
      "Step 99800: Train Loss 0.017069168388843536 | Best Val Loss 0.021282896399497986\n",
      "Step 99900: Train Loss 0.018183454871177673 | Best Val Loss 0.021282896399497986\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    Xb, Yb = get_batch(Xtr, Ytr, batch_size)\n",
    "\n",
    "    Xb = Xb.to('cuda')\n",
    "    Yb = Yb.to('cuda')\n",
    "\n",
    "    loss = criterion(model(Xb), Yb)\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        val_loss = criterion(model(Xval.cuda()), Yval.cuda())\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_val.pth')\n",
    "\n",
    "        print(f\"Step {i}: Validation Loss {val_loss.item()}\")\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'Step {i}: Train Loss {loss.item()} | Best Val Loss {best_val_loss.item()}')\n",
    "        if loss < best_train_loss:\n",
    "            best_train_loss = loss\n",
    "            torch.save(model.state_dict(), 'best_model_train.pth')\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LTVDynamicsModel(\n",
       "  (time_varying_F): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (5): Linear(in_features=128, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_val.pth', weights_only=True))\n",
    "model.cpu()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = get_data('data/00001.csv')[:100]\n",
    "test_data = test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors= []\n",
    "\n",
    "alpha = 0.0\n",
    "x0 = torch.from_numpy(test_data[0, None]).float()\n",
    "x0 = (x0 - mean) / std\n",
    "dx = model(x0).detach()\n",
    "# x1 = x1 * std[3] + mean[3]\n",
    "rollout = [x0[0,3] * std[3] + mean[3], (x0[0,3] + dx[0]) * std[3] + mean[3]] # lateral accelerations [x0[0,3], x1[0]] x0[0, 3] * std[3] + mean[3]\n",
    "for t in range(1, test_data.shape[0]-1):\n",
    "    xt = torch.from_numpy(test_data[t, None]).float()\n",
    "    xt[:, 3] = (1-alpha) * rollout[-1] + alpha * rollout[-2]\n",
    "    xt = (xt - mean) / std\n",
    "    dx = model(xt).detach()\n",
    "    # xt1 = xt1 * std[3] + mean[3]\n",
    "    xt1 = xt[0, 3] + dx[0]\n",
    "    rollout.append(xt1* std[3] + mean[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_rollout(rollout, test_data):\n",
    "  fig, ax = plt.subplots(figsize=(10, 5))\n",
    "  ax.plot(test_data[:, 3], label=\"GT Lat Acc\", alpha=0.5)\n",
    "  ax.plot([latacc for latacc in rollout], label=\"Model\", alpha=0.5)\n",
    "  ax.legend()\n",
    "  ax.set_xlabel(\"Step\")\n",
    "  ax.set_ylabel(\"Lateral Acceleration\")\n",
    "  ax.set_title(\"Rollout\")\n",
    "  plt.show()\n",
    "\n",
    "plot_rollout(rollout, test_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
